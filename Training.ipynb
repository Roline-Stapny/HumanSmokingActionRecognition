{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Training.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOikIZJGBmNkj7jrIi27zG4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vfQLr2NZ5_u2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610566108713,"user_tz":360,"elapsed":714758,"user":{"displayName":"Roline Saldanha","photoUrl":"","userId":"08007127351234095824"}},"outputId":"61c676c6-d88c-4769-8229-6a6a4d829fd2"},"source":["from google.colab import drive\n","drive.flush_and_unmount\n","drive.mount(\"/content/drive/\", force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kwO1EtjjL3O1","executionInfo":{"status":"ok","timestamp":1610566113851,"user_tz":360,"elapsed":3746,"user":{"displayName":"Roline Saldanha","photoUrl":"","userId":"08007127351234095824"}},"outputId":"d2e90643-9995-4157-aee9-692a9d60d417"},"source":["!pip install scikit-video"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting scikit-video\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/a6/c69cad508139a342810ae46e946ebb3256aa6e42f690d901bb68f50582e3/scikit_video-1.1.11-py2.py3-none-any.whl (2.3MB)\n","\u001b[K     |████████████████████████████████| 2.3MB 9.2MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from scikit-video) (1.4.1)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from scikit-video) (7.0.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-video) (1.19.5)\n","Installing collected packages: scikit-video\n","Successfully installed scikit-video-1.1.11\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3qovcZbBfFNb","executionInfo":{"status":"ok","timestamp":1610567619046,"user_tz":360,"elapsed":5540,"user":{"displayName":"Roline Saldanha","photoUrl":"","userId":"08007127351234095824"}}},"source":["#This section contains all the necessary functions that are required while training the model\n","\n","import time\n","import os\n","import pickle\n","import cv2\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as manimation\n","import matplotlib.patheffects as pe\n","import matplotlib.patches as mpatches\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","import math\n","from tqdm import tqdm\n","# from tqdm import tnrange, tqdm_notebook #used when I run in colab/GCloud\n","from random import sample\n","import numpy as np\n","from collections import Counter\n","import sys\n","from PIL import Image\n","import glob\n","\n","def set_project_folder_dir(if_open_new_folder, local_dir, use_model_folder_dir=False, mode=None):\n","    if use_model_folder_dir:\n","        folder_dir = os.path.join(os.path.normpath(local_dir + os.sep + os.pardir), mode)\n","        create_folder_dir_if_needed(folder_dir)\n","    else:\n","        if if_open_new_folder != 'False':\n","            folder_dir = open_new_folder(if_open_new_folder, local_dir)\n","        else:\n","            folder_dir = local_dir\n","    return folder_dir\n","\n","\n","\n","def open_new_folder(if_open_new_folder, local_dir):\n","    if if_open_new_folder == 'True':\n","        folder_name = time.strftime(\"%Y%m%d-%H%M%S\")\n","    else:\n","        folder_name = 'debug'\n","    folder_dir = os.path.join(local_dir, folder_name)\n","    create_folder_dir_if_needed(folder_dir)\n","    return folder_dir\n","\n","\n","def save_setting_info(args, device, folder_dir):\n","    setting_file_name = os.path.join(folder_dir, 'setting_info.txt')\n","    args_dict = args.__dict__\n","    with open(setting_file_name, 'w') as f:\n","        for key, value in args_dict.items():\n","            f.write(key + ' : ' + str(value) + '\\n')\n","        f.write(str(device))\n","\n","\n","def plot_label_distribution(dataloaders, folder_dir, load_all_data_to_RAM_mode, label_decoder_dict, mode='train'):\n","    if mode == 'train':\n","        datasets = [dataloaders[dataloader_name].dataset for dataloader_name in dataloaders.keys()]\n","        plot_distribution(datasets, list(dataloaders.keys()), load_all_data_to_RAM_mode, folder_dir, label_decoder_dict)\n","    else:\n","        plot_distribution([dataloaders.dataset], ['test'], load_all_data_to_RAM_mode, folder_dir, label_decoder_dict)\n","\n","\n","def plot_distribution(datasets_list, dataset_names_list, load_all_data_to_RAM_mode, folder_dir, label_decoder_dict):\n","    plt.figure(figsize=(10, 6))\n","    for index, dataset in enumerate(datasets_list):\n","        if load_all_data_to_RAM_mode:\n","            counter_occurrence_of_each_class = Counter(dataset.tensors[1].tolist())\n","        else:\n","            counter_occurrence_of_each_class = Counter(dataset.labels)\n","        with open(os.path.join(folder_dir, 'frequency_of_each_class_{}.pkl'.format(dataset_names_list[index])), 'wb') as f:\n","            pickle.dump(counter_occurrence_of_each_class, f, pickle.HIGHEST_PROTOCOL)\n","        sorted_counter = sorted(counter_occurrence_of_each_class.items())\n","        x, y = zip(*sorted_counter)\n","        plt.bar(x, y)\n","    plt.legend(dataset_names_list)\n","    plt.title('The frequency of each class\\n' + '&'.join(dataset_names_list))\n","    plt.xlabel('label')\n","    plt.ylabel('Frequency')\n","    x_ticks_labels = [label_decoder_dict[label_code+1] for label_code in x]\n","    plt.xticks(x, x_ticks_labels, fontsize=8, rotation=90)\n","    plt.yticks(fontsize=8)\n","    plt.tight_layout()\n","    plt.xlim(-1, max(x) + 1)\n","    plt.savefig(os.path.join(folder_dir, '_'.join(dataset_names_list) + '.jpg'), dpi=300, bbox_inches=\"tight\")\n","    plt.close()\n","\n","\n","\n","def split_data(ucf_list_root, seed, number_of_classes, split_size, folder_dir):\n","    print(ucf_list_root, folder_dir)\n","    video_names_train, video_names_test, labels, labels_decoder_dict = get_video_list(ucf_list_root, number_of_classes, folder_dir)\n","    print(\"hello\")\n","    print(video_names_train)\n","    video_names_train, video_names_val, labels_train, labels_val = train_test_split(video_names_train, labels,\n","                                                                                    test_size=split_size,\n","                                                                                    random_state=seed)\n","    \n","    save_video_names_test_and_add_labels(video_names_test, labels_decoder_dict, folder_dir, number_of_classes)\n","    # save labels_decoder_dict\n","    with open(os.path.join(folder_dir, 'labels_decoder_dict.pkl'), 'wb') as f:\n","        pickle.dump(labels_decoder_dict, f, pickle.HIGHEST_PROTOCOL)\n","    return [video_names_train, labels_train], [video_names_val, labels_val], labels_decoder_dict\n","\n","\n","def get_data(mode, video_names, list, number_of_classes, labels=[]):\n","    # setting the data files as a list so the not overpower the system\n","    for video_name in video_names:\n","        if mode == 'train':\n","            label = video_name.split('-')[0]\n","            video_name =  video_name\n","\n","            label = int(label.rstrip('\\n'))\n","            if number_of_classes is None or label in range(1, number_of_classes + 1):\n","                labels.append(label-1)\n","                list.append(video_name.split('.')[0])\n","            else:\n","                continue\n","        else:\n","            list.append(video_name.split('.')[0])\n","    return list, labels\n","\n","\n","def get_video_list(ucf_list_root, number_of_classes, folder_dir):\n","    # ====== get a list of video names ======\n","    video_names_train, video_names_test, labels = [], [], []\n","    sample_train_test_split = str(sample(range(1, 4), 1)[0])\n","    with open(os.path.join(folder_dir, 'setting_info.txt'), 'a+') as f:\n","        f.write('\\nThe test/train split that we have train on is {}'.format(sample_train_test_split))\n","\n","      \n","        videopath = glob.glob(ucf_list_root+\"*.avi\")\n","        print(\"Videopath\", videopath)\n","        video_names = []\n","        for path in videopath:\n","          video_names.append(path.split('/')[-1])\n","\n","\n","        \n","        video_names_train, labels = get_data('train', video_names, video_names_train, number_of_classes, labels)\n","        labels_decoder_dict = {1 : \"Smoking\", 2 : \"Not Smoking\"}\n","\n","        video_names_test, _ = get_data('test', video_names, video_names_test, number_of_classes)\n","\n","       \n","    return video_names_train, video_names_test, labels, labels_decoder_dict\n","\n","\n","def save_video_names_test_and_add_labels(video_names_test, labels_decoder_dict, folder_dir, number_of_classes):\n","    save_test_video_details = os.path.join(folder_dir, 'test_videos_detailes.txt')\n","    with open(save_test_video_details, 'w') as f:\n","        for text_video_name in video_names_test:\n","            label_string = text_video_name.split('/')[0]\n","            # endoce label\n","            for key, value in labels_decoder_dict.items():\n","                if value == label_string:\n","                    label_code = key\n","                else:\n","                    continue\n","                if number_of_classes is None or label_code in range(0, number_of_classes):\n","                    f.write(text_video_name + ' ' + str(label_code) + '\\n')\n","                else:\n","                    continue\n","\n","\n","def plot_images_with_predicted_labels(local_x, label_decoder_dict, predicted_labels, folder_dir, epoch):\n","    folder_save_images = os.path.join(folder_dir, 'Images')\n","    create_folder_dir_if_needed(folder_save_images)\n","    n_rows = math.trunc(math.sqrt(len(local_x)))\n","    n_cols = n_rows\n","    if n_rows == 1 and n_cols == 1:\n","        plot_single_images_with_predicted_labels(local_x, label_decoder_dict, predicted_labels, folder_save_images, epoch)\n","    else:\n","        fig, ax = plt.subplots(ncols=n_cols, nrows=n_rows, figsize=(10, 10))\n","        for row in range(n_rows):\n","            for col in range(n_cols):\n","                img = local_x[col + (row * n_cols)][0].permute(1, 2, 0)\n","                img_scale = (img - img.min()) / (img.max() - img.min())\n","                ax[row, col].imshow(img_scale)\n","                label_for_title = label_decoder_dict[predicted_labels[col + (row * n_cols)].item()+1]\n","                #label_for_title = str(predicted_labels[col + (row * n_cols)].item())\n","                ax[row, col].set_title(label_for_title)\n","                ax[row, col].set_xticks([])\n","                ax[row, col].set_yticks([])\n","        plt.savefig(os.path.join(folder_save_images, 'predicted_labels {} epoch.png'.format(epoch)))\n","        plt.close()\n","\n","\n","def plot_single_images_with_predicted_labels(local_x, label_decoder_dict, predicted_labels, folder_save_images, epoch):\n","    fig, ax = plt.subplots(figsize=(10, 10))\n","    img = local_x[0][0].permute(1, 2, 0)\n","    img_scale = (img - img.min()) / (img.max() - img.min())\n","    ax.imshow(img_scale)\n","    \n","    label_for_title = label_decoder_dict[predicted_labels[0].item()+1]\n","    ax.set_title(label_for_title)\n","    ax.set_xticks([])\n","    ax.set_yticks([])\n","    plt.savefig(os.path.join(folder_save_images, 'predicted_labels {} epoch.png'.format(epoch)))\n","    plt.close()\n","\n","\n","def create_folder_dir_if_needed(folder_save_dir):\n","    if not os.path.exists(folder_save_dir):\n","        os.makedirs(folder_save_dir)\n","\n","\n","\n","def load_all_dataset_to_RAM(dataloaders, dataset_order, batch_size):\n","    images_train, labels_train, images_val, labels_val = [], [], [], []\n","    for i, mode in enumerate(['train', 'val']):\n","        images_list = [images_train, images_val][i]\n","        labels_list = [labels_train, labels_val][i]\n","        with tqdm(total=len(dataloaders[mode])) as pbar:\n","            # with tqdm_notebook(total=len(dataloaders[mode])) as pbar:\n","            for local_images, local_label in dataloaders[mode]:\n","                images_list += [local_images]\n","                labels_list += [local_label]\n","                pbar.update(1)\n","    images_train = torch.cat(images_train, axis=0)\n","    labels_train = torch.cat(labels_train, axis=0)\n","    images_val = torch.cat(images_val, axis=0)\n","    labels_val = torch.cat(labels_val, axis=0)\n","    datasets = {dataset_order[index]: TensorDataset(x[0], x[1]) for index, x in\n","                enumerate([[images_train, labels_train], [images_val, labels_val]])}\n","    dataloaders = {x: DataLoader(datasets[x], batch_size=batch_size, shuffle=True)\n","                   for x in ['train', 'val']}\n","    return dataloaders\n","\n","\n","def load_all_dataset_to_RAM_test(dataloader, batch_size):\n","    images_test, labels_test = [], []\n","    with tqdm(total=len(dataloader)) as pbar:\n","        # with tqdm_notebook(total=len(dataloader)) as pbar:\n","        for local_images, local_label in dataloader:\n","            images_test += [local_images]\n","            labels_test += [local_label]\n","            pbar.update(1)\n","    images_test = torch.cat(images_test, axis=0)\n","    labels_test = torch.cat(labels_test, axis=0)\n","    dataset = TensorDataset(images_test, labels_test)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","    return dataloader\n","\n","\n","def foward_step_no_labels(model, images):\n","    # Must be done before you run a new batch. Otherwise the LSTM will treat a new batch as a continuation of a sequence\n","    model.Lstm.reset_hidden_state()\n","    with torch.no_grad():\n","        output = model(images)\n","    predicted_labels = output.detach().cpu().argmax(dim=1)\n","    return predicted_labels\n","\n","\n","def foward_step(model, images, labels, criterion, mode=''):  # predections\n","    # Must be done before you run a new batch. Otherwise the LSTM will treat a new batch as a continuation of a sequence\n","    model.Lstm.reset_hidden_state()\n","    if mode == 'test':\n","        with torch.no_grad():\n","            output = model(images)\n","    else:\n","        output = model(images)\n","    loss = criterion(output, labels)\n","    # Accuracy calculation\n","    predicted_labels = output.detach().argmax(dim=1)\n","    acc = (predicted_labels == labels).cpu().numpy().sum()\n","    return loss, acc, predicted_labels.cpu()\n","\n","\n","def train_model(model, dataloader, device, optimizer, criterion):\n","    train_loss, train_acc = 0.0, 0.0\n","    model.train()\n","    with tqdm(total=len(dataloader)) as pbar:\n","        # with tqdm_notebook(total=len(dataloader)) as pbar:\n","        for local_images, local_labels, ___ in dataloader:\n","            local_images, local_labels = local_images.to(device), local_labels.to(device)\n","            optimizer.zero_grad()  # zero the parameter gradients\n","            loss, acc, ___ = foward_step(model, local_images, local_labels, criterion, mode='train')\n","            train_loss += loss.item()\n","            train_acc += acc\n","            loss.backward()  # compute the gradients\n","            optimizer.step()  # update the parameters with the gradients\n","            pbar.update(1)\n","    train_acc = 100 * (train_acc / dataloader.dataset.__len__())\n","    train_loss = train_loss / len(dataloader)\n","    return train_loss, train_acc\n","\n","\n","def test_model(model, dataloader, device, criterion, mode='test'):\n","    val_loss, val_acc = 0.0, 0.0\n","    model.eval()\n","    if mode == 'save_prediction_label_list':\n","        prediction_labels_list = []\n","        true_labels_list = []\n","    with tqdm(total=len(dataloader)) as pbar:\n","        # with tqdm_notebook(total=len(dataloader)) as pbar:\n","        for local_images, local_labels, indexs in dataloader:\n","                local_images, local_labels = local_images.to(device), local_labels.to(device)\n","                loss, acc, predicted_labels = foward_step(model, local_images, local_labels, criterion, mode='test')\n","                if mode == 'save_prediction_label_list':\n","                    prediction_labels_list += [predicted_labels.detach().cpu()]\n","                    true_labels_list += [local_labels.detach().cpu()]\n","                val_loss += loss.item()\n","                val_acc += acc\n","                pbar.update(1)\n","    val_acc = 100 * (val_acc / dataloader.dataset.__len__())\n","    val_loss = val_loss / len(dataloader)\n","    if mode == 'save_prediction_label_list':\n","        return val_loss, val_acc, prediction_labels_list, local_images.cpu(), true_labels_list, indexs\n","    else:\n","        return val_loss, val_acc, predicted_labels, local_images.cpu()\n","\n","\n","def test_model_continues_movie(model, dataloader, device, criterion, save_path, label_decoder_dict):\n","    val_loss, val_acc = 0.0, 0.0\n","    model.eval()\n","    # ====== choosing one random batch from the dataloader ======\n","    dataloader_iter = iter(dataloader)\n","    images, labels, ___ = next(dataloader_iter)\n","    predicted_labels_list = []\n","    # ===== create continues movie and labels tensor, with X frames from each movie ======\n","    # ===== and stack a sliding window of size 5 frames to new dim so they will act as batch ======\n","    num_frames_to_sample = images.shape[1]\n","    sliding_window_images, continues_labels, continues_movie = create_sliding_window_x_frames_size_dataset\\\n","        (images, labels, num_frames_to_sample)\n","    # ====== predict the label of each sliding window, use batches because of GPU memory ======\n","    for batch_boundaries in range(0, len(sliding_window_images), dataloader.batch_size):\n","        batch_images_to_plot = sliding_window_images[batch_boundaries: batch_boundaries + dataloader.batch_size].to(\n","            device)\n","        batch_labels = continues_labels[batch_boundaries: batch_boundaries + dataloader.batch_size].to(device)\n","        loss, acc, predicted_labels = foward_step(model, batch_images_to_plot, batch_labels, criterion, mode='test')\n","        predicted_labels_list += [predicted_labels]\n","        val_acc += acc\n","    predicted_labels = torch.cat(predicted_labels_list, axis=0)\n","    val_loss += loss.item()\n","    create_video_with_labels(save_path, 'Video_with_prediction_vs_true_labels.avi', continues_movie, continues_labels,\n","                             predicted_labels, label_decoder_dict, mode='continues_test_movie')\n","    save_path_plots = os.path.join(save_path, 'Plots')\n","    create_folder_dir_if_needed(save_path_plots)\n","    plot_sliding_window_prediction_for_each_frame(continues_labels, predicted_labels, save_path_plots,\n","                                                  label_decoder_dict, labels)\n","    plot_function_of_num_frames_in_window_on_prediction(continues_labels, predicted_labels, save_path_plots,\n","                                                        num_frames_to_sample)\n","    val_acc = 100 * (val_acc / len(sliding_window_images))\n","    val_loss = val_loss / len(dataloader)\n","    return val_loss, val_acc, predicted_labels, images.cpu()\n","\n","\n","\n","\n","\n","def create_sliding_window_x_frames_size_dataset(local_images, local_labels, num_frames_to_sample,\n","                                                dataset_type='UCF101'):\n","    \"\"\"\"\n","    This function would join all of the images in the batch to one long continues movie, which would be\n","    composed from num_batch human action movies (shape - num_batch*num_frames_to_sample, 3, 224, 224).\n","    Than, a sliding window of num_frames_to_sample would be passed on the continues movie,\n","    creating a stack of mini videos that can be used as an input to the LRCN network.\n","    (shape - (num_batch - num_frames_to_sample+1), num_of_frames_to_samples, 3, 224, 224)\n","    The label for each sliding window would be set according the majority of frames we have for each action,\n","    meaning if the sliding window has 3 frames from the first action and two from the next action, the label of the sliding\n","    window would be the first action\n","    \"\"\"\n","    # ===== create continues movie, with X frames from each movie ======\n","    if dataset_type == 'UCF101':\n","        local_images = local_images[:, :num_frames_to_sample]\n","        continues_frames = local_images.view(local_images.shape[0] * local_images.shape[1], local_images.shape[2],\n","                                             local_images.shape[3], local_images.shape[4])\n","    else:\n","        continues_frames = local_images\n","    sliding_window_images = []\n","    for num_frame in range(continues_frames.shape[0] - num_frames_to_sample + 1):\n","        # ===== normalize the frames according to the imagenet preprocessing =======\n","        sliding_window_images += [continues_frames[num_frame: num_frame + num_frames_to_sample]]\n","    sliding_window_images = torch.stack(sliding_window_images)\n","    continues_frames = continues_frames[:len(sliding_window_images)]\n","    if dataset_type == 'UCF101':\n","        # ==== create continues label tensor where each frame has its own label ======\n","        majority_of_num_of_frames = math.ceil(\n","            num_frames_to_sample / 2) if num_frames_to_sample % 2 != 0 else num_frames_to_sample / 2 + 1\n","        mid_continues_labels = local_labels[1:len(local_labels) - 1].view(-1, 1).repeat(1, num_frames_to_sample).view(\n","            -1)\n","        start_continues_labels = local_labels[0].view(-1, 1).repeat(1, majority_of_num_of_frames).view(-1)\n","        end_continues_labeels = local_labels[-1].view(-1, 1).repeat(1, majority_of_num_of_frames).view(-1)\n","        continues_labels = torch.cat((start_continues_labels, mid_continues_labels, end_continues_labeels))\n","        return sliding_window_images, continues_labels, continues_frames\n","    else:\n","        return sliding_window_images\n","\n","\n","def plot_function_of_num_frames_in_window_on_prediction(continues_labels, predicted_labels, save_path_plots,\n","                                                        num_frames_to_sample):\n","    mean_acc_array = []\n","    for num_frames in range(num_frames_to_sample):\n","        predicted_labels_with_num_frames_in_window = np.array(\n","            [predicted_labels[i] for i in range(num_frames, len(predicted_labels), num_frames_to_sample)])\n","        labels_with_num_frames = np.array(\n","            [continues_labels[i] for i in range(num_frames, len(continues_labels), num_frames_to_sample)])\n","        mean_acc_array += [(predicted_labels_with_num_frames_in_window == labels_with_num_frames).sum() / len(\n","            labels_with_num_frames) * 100]\n","    mean_acc_array.reverse()\n","    x_axis = np.arange(num_frames_to_sample)\n","    plt.plot(x_axis, mean_acc_array, linestyle='-', marker=\"o\")\n","    plt.xticks(x_axis, np.arange(num_frames_to_sample, 0, -1))\n","    plt.xlabel('Number of frames from a specific human action')\n","    plt.ylabel('Mean accuracy [%]')\n","    plt.ylim(0, 100)\n","    plt.title('Change in accuracy with the change in frame num')\n","    plt.savefig(os.path.join(save_path_plots, 'analysis_of_predicted_labels_in_sliding_window.png'), dpi=300,\n","                bbox_inches='tight')\n","    plt.close()\n","\n","\n","def save_loss_info_into_a_file(train_loss, val_loss, train_acc, val_acc, folder_dir, epoch):\n","    file_name = os.path.join(folder_dir, 'loss_per_epoch.txt')\n","    with open(file_name, 'a+') as f:\n","        f.write('Epoch {} : Train loss {:.8f}, Train acc {:.4f}, Val loss {:.8f}, Val acc {:.4f}\\n'\n","                .format(epoch, train_loss, train_acc, val_loss, val_acc))\n","\n","\n","def set_transforms(mode):\n","    if mode == 'train':\n","        transform = transforms.Compose(\n","            [transforms.Resize(256),  # this is set only because we are using Imagenet pre-train model.\n","             transforms.RandomCrop(224),\n","             transforms.RandomHorizontalFlip(),\n","             transforms.ToTensor(),\n","             transforms.Normalize(mean=(0.485, 0.456, 0.406),\n","                                  std=(0.229, 0.224, 0.225))\n","             ])\n","    elif mode == 'test' or mode == 'val':\n","        transform = transforms.Compose([transforms.Resize((224, 224)),\n","                                        transforms.ToTensor(),\n","                                        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n","                                                             std=(0.229, 0.224, 0.225))])\n","    return transform\n","\n","\n","def create_new_video(save_path, video_name, image_array):\n","    \n","    (h, w) = image_array[0].shape[:2]\n","    if len(video_name.split('/')) > 1:\n","        video_name = video_name.split('/')[1]\n","    else:\n","        video_name = video_name.split('.mp4')[0]\n","        video_name = video_name + '.avi'\n","    save_video_path = os.path.join(save_path, video_name)\n","    output_video = cv2.VideoWriter(save_video_path, cv2.VideoWriter_fourcc(*'MJPG'), 5, (w, h), True)\n","    for frame in range(len(image_array)):\n","        output_video.write(image_array[frame])\n","    output_video.release()\n","    cv2.destroyAllWindows()\n","\n","\n","def create_video_with_labels(save_path, video_name, image_array, continues_labels, predicted_labels, label_decoder_dict,\n","                             video_original_size=None, fps=2.5, mode='single_movie'):\n","    if mode == 'single_movie':\n","        predicted_labels = torch.tensor(predicted_labels)\n","        predicted_labels = predicted_labels.view(-1, 1).repeat(1, len(image_array)).view(-1)\n","    path_save_videos = os.path.join(save_path, 'Videos')\n","    create_folder_dir_if_needed(path_save_videos)\n","    dpi = 300\n","    w, h = setting_video_size(video_original_size)\n","    image_array = F.interpolate(image_array, size=(h, w))\n","    image_array = image_array.transpose(2, 1).transpose(2, 3)\n","    n_frames = len(image_array)\n","    figure_size_w = round((w - 50) / float(dpi) * 2)\n","    figure_size_h = round(h / float(dpi) * 3)\n","    h_fig = plt.figure(figsize=(figure_size_w, figure_size_h), dpi=dpi)\n","    # ====== plot frame, would change with every frame ======\n","    if mode != 'youtube':\n","        h_ax = h_fig.add_axes([0.08, 0.25, 0.85, 0.8])\n","    else:\n","        h_ax = h_fig.add_axes([0.03, 0.1, 0.95, 0.95])\n","    img = (image_array[0] - image_array[0].min()) / (image_array[0].max() - image_array[0].min())\n","    h_im = h_ax.matshow(img)\n","    h_ax.set_axis_off()\n","    h_im.set_interpolation('none')\n","    h_ax.set_aspect('equal')\n","    # ======== plot the label prediction with the frame =====\n","    if mode != 'youtube':\n","        h_ax_plot = h_fig.add_axes([0.08, 0.25, 0.85, 0.05])\n","    else:\n","        h_ax_plot = h_fig.add_axes([0.03, 0.25, 0.95, 0.04])\n","    # h_ax_plot = h_fig.add_axes([0.1, 0.22, 0.8, 0.06])\n","    x_array = np.arange(len(predicted_labels)) + 0.5\n","    y_array = np.zeros(len(x_array))\n","    bool_array = None if continues_labels is None else continues_labels == predicted_labels\n","    color_dict = create_color_dict(predicted_labels)\n","    color_list = []\n","    h_text_object = set_text_to_video_frame(continues_labels, label_decoder_dict,\n","                                            predicted_labels, mode, bool_array=bool_array)\n","\n","    FFMpegWriter = manimation.writers['ffmpeg']\n","    metadata = dict(title=video_name, artist='Matplotlib')\n","    writer = FFMpegWriter(fps=fps, metadata=metadata)\n","    with writer.saving(h_fig, os.path.join(path_save_videos, video_name), dpi=dpi):  # change from 600 dpi\n","        for i in range(n_frames):\n","            set_text_to_video_frame(continues_labels, label_decoder_dict,\n","                                    predicted_labels, mode, h_text_object=h_text_object, bool_array=bool_array, frame=i)\n","            img = (image_array[i] - image_array[i].min()) / (image_array[i].max() - image_array[i].min())\n","            h_im.set_array(img)\n","            if i > 0:\n","                h_im_2.remove()\n","            y_array[:i + 1] = 1\n","            if mode != 'continues_test_movie':\n","                color_list += [color_dict[predicted_labels[i].item()]]\n","            else:\n","                color_list += ['green' if bool_array[i].item() else color_dict[predicted_labels[i].item()]]\n","            h_im_2 = h_ax_plot.bar(x_array, y_array, color=color_list, width=1.0)\n","            h_ax_plot.get_yaxis().set_ticks([])\n","            h_ax_plot.set_ylim(0, 1)\n","            h_ax_plot.tick_params(axis=\"x\", labelsize=4)\n","            h_ax_plot.set_xlim(0, len(x_array))\n","            writer.grab_frame()\n","    plt.close()\n","\n","\n","def setting_sample_rate(num_frames_to_extract, sampling_rate, video, fps, ucf101_fps):\n","    video.set(cv2.CAP_PROP_POS_AVI_RATIO, 1)\n","    video_length = video.get(cv2.CAP_PROP_POS_MSEC) / 1000\n","    num_frames = int(video_length * fps)\n","    if num_frames_to_extract == 'all':\n","        sample_start_point = 0\n","        if fps != ucf101_fps and sampling_rate != 0:\n","            sampling_rate = math.ceil(fps / (ucf101_fps / sampling_rate))\n","    elif video_length < (num_frames_to_extract * sampling_rate):\n","        sample_start_point = 0\n","        sampling_rate = 2\n","    else:\n","        sample_start_point = sample(range(num_frames - (num_frames_to_extract * sampling_rate)), 1)[0]\n","    return sample_start_point, sampling_rate, num_frames\n","\n","\n","\n","def plot_confusion_matrix(predicted_labels, true_labels, label_decoder_dict, save_path):\n","    class_order_to_plot = list(label_decoder_dict.keys())[:true_labels.max() + 1]\n","    cm = confusion_matrix(true_labels, predicted_labels, labels=class_order_to_plot, normalize='true')\n","    # ==== plot the cm as heatmap ======\n","    plt.figure(figsize=(8, 6))\n","    plt.imshow(cm, interpolation='none', aspect='auto', cmap=plt.cm.Blues)\n","    cb = plt.colorbar()\n","    cb.ax.tick_params(labelsize=10)\n","    x_labels = [label_decoder_dict[label_code] for label_code in class_order_to_plot]\n","    plt.xticks(class_order_to_plot, x_labels, rotation=90, fontsize=6)\n","    plt.yticks(class_order_to_plot, x_labels, fontsize=6)\n","    plt.ylim(len(class_order_to_plot), -0.5)\n","    plt.title('Normalized confusion matrix')\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(save_path, 'Normalized_confusion_matrix.png'), dpi=300, bbox_inches='tight')\n","    plt.close()\n","\n","\n","def plot_acc_per_class(predicted_labels, true_labels, label_decoder_dict, save_path):\n","    # ===== count the number of times each class appear in the test data =====\n","    frequency_of_each_class = Counter(true_labels.tolist())\n","    # ===== load the frequency counter for the train dataset, would be used to mark low frequency classes =====\n","    global_dir = os.path.normpath(save_path + os.sep + os.pardir + os.sep + os.pardir)\n","    with open(os.path.join(global_dir, 'frequency_of_each_class_train.pkl'), 'rb') as f:\n","        frequency_of_each_class_train = pickle.load(f)\n","    # ===== count the number of times each class is labeled correctly =======\n","    class_list = list(label_decoder_dict.keys())[: true_labels.max() + 1]\n","    acc = true_labels == predicted_labels\n","    counter_correct_labeled = Counter()\n","    for index, true_label in enumerate(true_labels):\n","        counter_correct_labeled[true_label.item()] += acc[index].item()\n","    # ==== calculate the accuracy to predict each class =====\n","    acc_per_class = []\n","    mean_frequency = sum(list(frequency_of_each_class_train.values())) / len(frequency_of_each_class_train)\n","    classes_with_lower_frequency_compare_to_average = []\n","    for class_ in class_list:\n","        acc_per_class += [counter_correct_labeled[class_] / frequency_of_each_class[class_] * 100]\n","        if frequency_of_each_class_train[class_] <= (0.9 * mean_frequency):\n","            classes_with_lower_frequency_compare_to_average += [class_]\n","    acc_classes_with_lower_frequency_compare_to_average = [acc_per_class[class_] for class_ in\n","                                                           classes_with_lower_frequency_compare_to_average]\n","    plt.figure(figsize=(10, 10))\n","    plt.bar(class_list, acc_per_class)\n","    plt.bar(classes_with_lower_frequency_compare_to_average, acc_classes_with_lower_frequency_compare_to_average,\n","            color='red')\n","    x_labels = [label_decoder_dict[label_code] for label_code in class_list]\n","    plt.xticks(class_list, x_labels, rotation=90, fontsize=12)\n","    plt.yticks(fontsize=12)\n","    plt.xlabel('Classes', fontsize=16)\n","    plt.ylabel('Accuracy [%]', fontsize=16)\n","    plt.xlim(-1, class_list[-1] + 1)\n","    plt.ylim(0, 109)\n","    plt.legend(['freq > 0.9 * avr freq of a class', 'freq <= 0.9 * avr freq of a class'])\n","    plt.title('The accuracy score for each class', fontsize=18)\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(save_path, 'The_accuracy_score_for_each_class.png'), dpi=300, bbox_inches='tight')\n","    plt.close()\n","\n","\n","def check_if_batch_size_bigger_than_num_classes(batch_size, num_of_classes):\n","    if num_of_classes is None:\n","        num_of_classes = 101\n","    if batch_size > num_of_classes:\n","        print(\n","            'Your batch size is bigger than the num of classes you are testing. This would cause an Error in the custom sampler. Your options are:\\n'\n","            '1. Reduce the batch size so it would be smaller or equal to the number of classes you are testing.\\n'\n","            '2. Reduce the number of classes so it would be bigger or equal to the batch size.\\n'\n","            '3. Stop using the custom sampler: erase the sampler parameter from the dataloader and change the shuffle '\n","            'parameter to True.')\n","        sys.exit()\n","\n","\n","def plot_sliding_window_prediction_for_each_frame(continues_labels, predicted_labels, save_path_plots,\n","                                                  label_decoder_dict, original_order_of_labels):\n","    max_label_code = max(max(predicted_labels).item(), max(continues_labels).item())\n","    predicted_labels_one_hot = create_one_hot_vector_matrix(predicted_labels.numpy(), max_label_code)\n","    labels_one_hot = create_one_hot_vector_matrix(continues_labels.numpy(), max_label_code)\n","    labels_one_hot = labels_one_hot * 2\n","    one_hot_matrix_to_plot = predicted_labels_one_hot + labels_one_hot\n","    one_hot_matrix_to_plot, labels_new_order = resort_matrix(original_order_of_labels, one_hot_matrix_to_plot)\n","    one_hot_matrix_to_plot = one_hot_matrix_to_plot[~np.all(one_hot_matrix_to_plot == 0, axis=1)]\n","    one_hot_matrix_to_plot = np.apply_along_axis(increase_the_error_value_for_non_neighbors_labels, 0,\n","                                                 one_hot_matrix_to_plot)\n","    plt.figure(figsize=(12, 10))\n","    if 5 not in np.unique(one_hot_matrix_to_plot):\n","        one_hot_matrix_to_plot = np.vstack((one_hot_matrix_to_plot, np.full((1, one_hot_matrix_to_plot.shape[1]), 5)))\n","        im = plt.imshow(one_hot_matrix_to_plot[:-1,:], cmap='bwr', aspect='auto')\n","        values = ['None', 'Predicted_labels_next_movie', 'true_label', 'predicted_label_is_true_label']\n","    else:\n","        im = plt.imshow(one_hot_matrix_to_plot, cmap='bwr', aspect='auto')\n","        values = ['None', 'Predicted_labels_next_movie', 'true_label', 'predicted_label_is_true_label', 'Predicted_label_errors']\n","    skip_x_ticks = math.ceil(len(continues_labels) / 15)\n","    x_array = np.arange(0, len(continues_labels), skip_x_ticks)\n","    y_labels = [label_decoder_dict[label_code] for label_code in labels_new_order]\n","    plt.ylim(len(y_labels), -0.3)\n","    plt.xticks(x_array, x_array, fontsize=10)\n","    plt.yticks(np.arange(len(labels_new_order)), y_labels, fontsize=10)\n","    # ==== create coustomize legand to the heat map =====\n","    colors = [im.cmap(im.norm(value)) for value in range(len(values))]\n","    patches = [mpatches.Patch(color=colors[i], label=values[i], edgecolor='b') for i in\n","               range(len(values))]\n","    plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.5, frameon=True)\n","    plt.title('Label Prediction in each frame', fontsize=14)\n","    plt.savefig(os.path.join(save_path_plots, 'change_in_accuracy_with_the_movment_of_sliding_window.png'), dpi=300,\n","                bbox_inches='tight')\n","    plt.close()\n","\n","\n","def create_one_hot_vector_matrix(array, array_max):\n","    one_hot_array = np.zeros((array.size, array_max + 1))\n","    one_hot_array[np.arange(array.size), array] = 1\n","    one_hot_array = one_hot_array.transpose()\n","    return one_hot_array\n","\n","\n","def resort_matrix(labels_order, matrix):\n","    sorted_matrix = np.zeros(matrix.shape)\n","    classes_that_we_plotted = []\n","    for row_index, label in enumerate(labels_order):\n","        if label.item() in classes_that_we_plotted:\n","            pass\n","        else:\n","            sorted_matrix[row_index] = matrix[label.item()]\n","            classes_that_we_plotted += [label.item()]\n","    index_of_filled_rows = row_index + 1\n","    for index in range(len(matrix)):\n","        if index in classes_that_we_plotted:\n","            pass\n","        else:\n","            sorted_matrix[index_of_filled_rows] = matrix[index]\n","            index_of_filled_rows += 1\n","            if np.nonzero(matrix[index])[0].size != 0 and index not in classes_that_we_plotted:\n","                classes_that_we_plotted += [index]\n","    return sorted_matrix, classes_that_we_plotted\n","\n","\n","def increase_the_error_value_for_non_neighbors_labels(matrix_col):\n","    indices_of_non_zero_elements = np.nonzero(matrix_col)\n","    if len(indices_of_non_zero_elements[0]) > 1:\n","        dist_between_indices = indices_of_non_zero_elements[0][1] - indices_of_non_zero_elements[0][0]\n","        if dist_between_indices > 1:\n","            matrix_col[matrix_col == 1] = 5\n","    return matrix_col\n","\n","\n","def print_dataset_type_error():\n","    print(\n","        'You have enter a wrong dataset type in the dataset function. please fix it. possabilites are youtube or UCF101(the default)')\n","    sys.exit()\n","\n","\n","def plot_sliding_window_prediction_for_each_frame_no_labels(predicted_labels, save_path_plots, label_decoder_dict):\n","    original_order_of_labels = []\n","    for label in predicted_labels:\n","        if label.item() in original_order_of_labels:\n","            pass\n","        else:\n","            original_order_of_labels += [label]\n","    one_hot_matrix_to_plot = create_one_hot_vector_matrix(predicted_labels.numpy(), max(predicted_labels).item())\n","    one_hot_matrix_to_plot, ____ = resort_matrix(original_order_of_labels, one_hot_matrix_to_plot)\n","    one_hot_matrix_to_plot = one_hot_matrix_to_plot[~np.all(one_hot_matrix_to_plot == 0, axis=1)]\n","    fig, ax = plt.subplots(figsize=(12, 10))\n","    im = ax.imshow(one_hot_matrix_to_plot, cmap='GnBu', aspect='auto')\n","    skip_x_ticks = math.ceil(len(predicted_labels) / 15)\n","    x_array = np.arange(0, len(predicted_labels), skip_x_ticks)\n","    y_labels = [label_decoder_dict[label_code.item()] for label_code in original_order_of_labels]\n","    y_ticks = np.arange(len(original_order_of_labels))\n","    ax.set_ylim(len(y_labels), -0.3)\n","    ax.set_xticklabels(x_array, fontsize=10)\n","    ax.set_yticks(y_ticks)\n","    ax.set_yticklabels(y_labels, fontsize=10)\n","    # ==== create coustomize legand to the heat map =====\n","    values = ['None', 'Predicted_labels']\n","    colors = [im.cmap(im.norm(value)) for value in range(len(values))]\n","    patches = [mpatches.Patch(color=colors[i], label=values[i], edgecolor='b') for i in\n","               range(len(values))]\n","    plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.5, frameon=True)\n","    plt.title('Label Prediction in each frame', fontsize=14)\n","    plt.savefig(os.path.join(save_path_plots, 'change_in_accuracy_with_the_movement_of_sliding_window.png'), dpi=300,\n","                bbox_inches='tight')\n","\n","\n","def print_error_preprocessing_movie_mode():\n","    print('Your value in the pre-processing movie mode is incorrect. your options are:\\n'\n","          '1. live pre-processing.\\n'\n","          '2. pre-processied movie. \\n'\n","          'please choose one of them')\n","    sys.exit()\n","\n","\n","def predict_labels_of_sliding_window(sliding_window_images, batch_size, device, model):\n","    predicted_labels_list = []\n","    for batch_boundaries in range(0, len(sliding_window_images), batch_size):\n","        batch_images_to_plot = sliding_window_images[batch_boundaries: batch_boundaries + batch_size].to(device)\n","        predicted_labels = foward_step_no_labels(model, batch_images_to_plot)\n","        predicted_labels_list += [predicted_labels]\n","    return torch.cat(predicted_labels_list, axis=0)\n","\n","\n","def set_text_to_video_frame(continues_labels, label_decoder_dict, predicted_labels, mode, h_text_object=None,\n","                            frame='start', bool_array=None):\n","    if frame == 'start':\n","        height = 0.07 if mode != 'youtube' else 0.12\n","        fontsize = 5 if mode!= 'youtube' else 8\n","        h_text_1 = plt.text(0.18, height, 'Predicted labels - {}'.format(label_decoder_dict[predicted_labels[0].item()]),\n","                            color='blue', fontsize=fontsize, transform=plt.gcf().transFigure)\n","        if continues_labels is not None:\n","            h_text_2 = plt.text(0.18, 0.11, 'Original_labels', color='black', fontsize=5,\n","                                transform=plt.gcf().transFigure)\n","            h_text_3 = plt.text(0.44, 0.01, 'True/False', color='red', fontsize=6, transform=plt.gcf().transFigure,\n","                                path_effects=[pe.withStroke(linewidth=1, foreground=\"black\")])\n","            return {index + 1: text_object for index, text_object in enumerate([h_text_1, h_text_2, h_text_3])}\n","        else:\n","            return {1: h_text_1}\n","    else:\n","        h_text_object[1].set_text('Predicted labels - {}'.format(label_decoder_dict[predicted_labels[frame].item()]))\n","        if continues_labels is not None:\n","            h_text_object[2].set_text('Original label - {}'.format(label_decoder_dict[continues_labels[frame].item()]))\n","            color = 'green' if bool_array[frame].item() else 'red'\n","            h_text_object[3].remove()\n","            h_text_object[3] = plt.text(0.44, 0.01, str(bool_array[frame].item()), color=color, fontsize=6,\n","                                        transform=plt.gcf().transFigure,\n","                                        path_effects=[pe.withStroke(linewidth=1, foreground=\"black\")])\n","\n","\n","def generate_list_of_colors(num_labels):\n","    green_color_codes = [[154, 205, 50], [85, 107, 47], [107, 142, 35], [124, 252, 0], [127, 255, 0], [173, 255, 47],\n","                         [0, 100, 0],\n","                         [0, 128, 0], [34, 139, 34], [0, 255, 0], [50, 205, 50], [144, 238, 144], [152, 251, 152],\n","                         [60, 179, 113],\n","                         [46, 139, 87], [0, 255, 127], [0, 250, 154]]\n","    color_list = []\n","    for i in range(num_labels):\n","        color = list(np.random.choice(range(256), size=3))\n","\n","        while color in color_list or color in green_color_codes:\n","            color = list(np.random.choice(range(256), size=3))\n","        color_norm = [single_color / 255 for single_color in color]\n","        color_list += [color_norm]\n","    color_list_tuple = [tuple(color_as_list) for color_as_list in color_list]\n","    return color_list_tuple\n","\n","\n","def create_color_dict(predicted_labels):\n","    unique_labels = predicted_labels.unique()\n","    color_list = generate_list_of_colors(len(unique_labels))\n","    color_dict = {}\n","    for index, label in enumerate(unique_labels):\n","        index_of_specific_label = (predicted_labels == label.item()).nonzero()\n","        if len(index_of_specific_label) > (0.5 * len(predicted_labels)):\n","            color_list[index] = 'green'\n","        color_dict[label.item()] = color_list[index]\n","    return color_dict\n","\n","\n","def save_video_original_size_dict(video_original_size_dict, save_path):\n","    with open(os.path.join(save_path, 'video_original_size_dict.pkl'), 'wb') as f:\n","        pickle.dump(video_original_size_dict, f, pickle.HIGHEST_PROTOCOL)\n","\n","\n","def load_and_extract_video_original_size(read_video_original_size_dir):\n","    with open(os.path.join(read_video_original_size_dir, 'video_original_size_dict.pkl'), 'rb') as f:\n","        dict = pickle.load(f)\n","    return dict\n","\n","\n","def setting_video_size(video_original_size):\n","    if video_original_size is None:\n","         (w, h) = (320, 240)\n","    else:\n","        (w, h) = (int(video_original_size[0]), int(video_original_size[1]))\n","    for size_element in [w, h]:\n","        if size_element % 2 == 0:\n","            size_element += 1\n","    return w, h"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"my4whAwSf74a","executionInfo":{"status":"ok","timestamp":1610566851047,"user_tz":360,"elapsed":3531,"user":{"displayName":"Roline Saldanha","photoUrl":"","userId":"08007127351234095824"}}},"source":["from torch.utils.data import Dataset\n","import torch\n","import numpy as np\n","from PIL import Image\n","import os\n","import skvideo\n","import skvideo.io\n","from torch.utils.data.sampler import Sampler\n","from random import sample\n","\n","class UCF101Dataset(Dataset):\n","    def __init__(self, data_path, data, mode, dataset='UCF101'):\n","        super(UCF101Dataset, self).__init__()\n","        self.dataset = dataset\n","        if self.dataset == 'UCF101':\n","            self.labels = data[1]\n","        self.data_path = data_path\n","        self.images = data[0]\n","        self.transform = set_transforms(mode)\n","\n","    # ====== Override to give PyTorch size of dataset ======\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        if self.dataset == 'UCF101':\n","            #sampled_video_name = self.images[idx].split('/')[1] +'.avi'\n","            sampled_video_name = self.images[idx] +'.avi'\n","            print(sampled_video_name)\n","        elif self.dataset == 'youtube':\n","            sampled_video_name = self.images[idx]\n","        else:\n","           print_dataset_type_error()\n","        # ====== extract numpy array from the video and sample it so we will have an array with lower FPS rate =======\n","        video_frames = skvideo.io.vread(os.path.join(self.data_path, sampled_video_name))\n","        video_frames_array = []\n","        for image in video_frames:\n","            img = Image.fromarray(image.astype('uint8'), 'RGB')\n","            img = self.transform(img)\n","            video_frames_array.append(img)\n","        img_stack = torch.stack(video_frames_array)\n","        #print(img_stack.shape)\n","        if self.dataset == 'UCF101':\n","            label = torch.from_numpy(np.asarray(int(self.labels[idx]))).long()\n","            return img_stack, label, idx\n","        else:\n","            return img_stack\n","\n","\n","\n","class UCF101DatasetSampler(Sampler):\n","    def __init__(self, data, batch_size):\n","        self.num_samples = len(data)\n","        self.classes_that_were_sampled = []\n","        self.data_labels = data.labels\n","        self.batch_size = batch_size\n","\n","\n","    def __iter__(self):\n","        idx_list = []\n","        for i in range(self.batch_size):\n","            idx_image_sample = sample(range(self.num_samples), 1)[0]\n","            label_sample = self.data_labels[idx_image_sample]\n","            while label_sample in self.classes_that_were_sampled:\n","                idx_image_sample = sample(range(self.num_samples), 1)[0]\n","                label_sample = self.data_labels[idx_image_sample]\n","            self.classes_that_were_sampled += [label_sample]\n","            idx_list += [idx_image_sample]\n","        return iter(idx_list)\n","\n","    def __len__(self):\n","        return self.num_samples"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"KWV1wpGK5zYw","executionInfo":{"status":"ok","timestamp":1610566851051,"user_tz":360,"elapsed":2649,"user":{"displayName":"Roline Saldanha","photoUrl":"","userId":"08007127351234095824"}}},"source":["# This sections contains the model network\n","\n","import torch.nn as nn\n","from torchvision import models\n","\n","class ConvLstm(nn.Module):\n","    def __init__(self, latent_dim, hidden_size, lstm_layers, bidirectional, n_class):\n","        super(ConvLstm, self).__init__()\n","        self.conv_model = Pretrained_conv(latent_dim)\n","        self.Lstm = Lstm(latent_dim, hidden_size, lstm_layers, bidirectional)\n","        self.output_layer = nn.Sequential(\n","            nn.Linear(2 * hidden_size if bidirectional==True else hidden_size, n_class),\n","            nn.Softmax()\n","            #nn.Softmax(dim=-1)\n","        )\n","\n","    def forward(self, x):\n","        batch_size, timesteps, channel_x, h_x, w_x = x.shape\n","        conv_input = x.view(batch_size * timesteps, channel_x, h_x, w_x)\n","        conv_output = self.conv_model(conv_input)\n","        lstm_input = conv_output.view(batch_size, timesteps, -1)\n","        lstm_output = self.Lstm(lstm_input)\n","        lstm_output = lstm_output[:, -1, :]\n","        output = self.output_layer(lstm_output)\n","        return output\n","\n","class Pretrained_conv(nn.Module):\n","    def __init__(self, latent_dim):\n","        super(Pretrained_conv, self).__init__()\n","        self.conv_model = models.resnet152(pretrained=True)\n","        # ====== freezing all of the layers ======\n","        for param in self.conv_model.parameters():\n","            param.requires_grad = False\n","        # ====== changing the last FC layer to an output with the size we need. this layer is un freezed ======\n","        self.conv_model.fc = nn.Linear(self.conv_model.fc.in_features, latent_dim)\n","\n","    def forward(self, x):\n","        return self.conv_model(x)\n","\n","class Lstm(nn.Module):\n","    def __init__(self, latent_dim, hidden_size, lstm_layers, bidirectional):\n","        super(Lstm, self).__init__()\n","        self.Lstm = nn.LSTM(latent_dim, hidden_size=hidden_size, num_layers=lstm_layers, batch_first=True, bidirectional=bidirectional)\n","        self.hidden_state = None\n","\n","    def reset_hidden_state(self):\n","        self.hidden_state = None\n","\n","    def forward(self,x):\n","        output, self.hidden_state = self.Lstm(x, self.hidden_state)\n","        return output"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"nFUfxv8R5hlD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610567643960,"user_tz":360,"elapsed":16857,"user":{"displayName":"Roline Saldanha","photoUrl":"","userId":"08007127351234095824"}},"outputId":"07d22c94-ed8f-4947-995a-c6b48856aa1c"},"source":["#Contains the hyperparameters and code to train the model\n","\n","import torch\n","import torch.nn as nn\n","import argparse\n","import time\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","import os\n","\n","\n","parser = argparse.ArgumentParser(description='UCF101 Action Recognition, LRCN architecture')\n","parser.add_argument('--epochs', default=15, type=int, help='number of total epochs')\n","parser.add_argument('--batch-size', default=32, type=int, help='mini-batch size (default:32)')\n","parser.add_argument('--lr', default=5e-4, type=float, help='initial learning rate (default:5e-4')\n","parser.add_argument('--num_workers', default=4, type=int,\n","                    help='initial num_workers, the number of processes that generate batches in parallel (default:4)')\n","parser.add_argument('--split_size', default=0.2, type=int, help='set the size of the split size between validation '\n","                                                                'data and train data')\n","\n","# Specify the path of preprocessed dataset here\n","parser.add_argument('--sampled_data_dir',\n","                    default=r'/content/drive/My Drive/directed studies/preprocessed_dataset_path/',\n","                    type=str, help='The dir for the sampled row data')\n","\n","\n","parser.add_argument('--seed', default=42, type=int,\n","                    help='initializes the pseudorandom number generator on the same number (default:42)')\n","parser.add_argument('--load_all_data_to_RAM', default=False, type=bool,\n","                    help='load dataset directly to the RAM, for faster computation. usually use when the num of class '\n","                         'is small (default:False')\n","parser.add_argument('--latent_dim', default=512, type=int, help='The dim of the Conv FC output (default:512)')\n","parser.add_argument('--hidden_size', default=256, type=int,\n","                    help=\"The number of features in the LSTM hidden state (default:256)\")\n","\n","parser.add_argument('--lstm_layers', default=2, type=int, help='Number of recurrent layers (default:2)')\n","parser.add_argument('--bidirectional', default=True, type=bool, help='set the LSTM to be bidirectional (default:True)')\n","parser.add_argument('--open_new_folder', default='True', type=str,\n","                    help='open a new folder for saving the run info, if false the info would be saved in the project '\n","                         'dir, if debug the info would be saved in debug folder(default:True)')\n","\n","parser.add_argument('--load_checkpoint', default=False, type=bool,\n","                    help='Loading a checkpoint and continue training with it')\n","parser.add_argument('--checkpoint_path', default='', type=str, help='Optional path to checkpoint model')\n","parser.add_argument('--checkpoint_interval', default=5, type=int, help='Interval between saving model checkpoints')\n","parser.add_argument('--val_check_interval', default=5, type=int, help='Interval between running validation test')\n","parser.add_argument('--local_dir',default='/content/drive/My Drive/directed studies/' , help='The local directory of the project, setting where to '\n","                                                             'save the results of the run') \n","parser.add_argument('--number_of_classes', default=2, type=int, help='The number of classes we would train on')\n","\n","def main():\n","    # ====== set the run settings ======\n","    args = parser.parse_args(\"\")\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","    folder_dir = set_project_folder_dir(args.open_new_folder, args.local_dir)\n","\n","    print('The setting of the run are:\\n{}\\n'.format(args))\n","    print('The training would take place on {}\\n'.format(device))\n","    print('The project directory is {}'.format(folder_dir))\n","\n","    save_setting_info(args, device, folder_dir)\n","    tensorboard_writer = SummaryWriter(folder_dir)\n","\n","    print('Initializing Datasets and Dataloaders...')\n","    train_data_names, val_data_names, label_decoder_dict = split_data(args.sampled_data_dir, args.seed,\n","                                                                      args.number_of_classes, args.split_size, folder_dir)\n","    print(label_decoder_dict)\n","    \n","    dataset_order = ['train', 'val']\n","    datasets = {dataset_order[index]: UCF101Dataset(args.sampled_data_dir, x, mode=dataset_order[index])\n","                for index, x in enumerate([train_data_names, val_data_names])}\n","    dataloaders = {x: DataLoader(datasets[x], batch_size=args.batch_size, shuffle=True)\n","                   for x in ['train', 'val']}\n","    # ======= if args.load_all_data_to_RAM True load dataset directly to the RAM (for faster computation) ======\n","    if args.load_all_data_to_RAM:\n","        dataloaders = load_all_dataset_to_RAM(dataloaders, dataset_order, args.batch_size)\n","\n","    plot_label_distribution(dataloaders, folder_dir, args.load_all_data_to_RAM, label_decoder_dict)\n","    print('Data prepared\\nLoading model...')\n","\n","    num_class = len(label_decoder_dict) if args.number_of_classes is None else args.number_of_classes\n","    model = ConvLstm(args.latent_dim, args.hidden_size, args.lstm_layers, args.bidirectional, num_class)\n","    #print(model)\n","    model = model.to(device)\n","    # ====== setting optimizer and criterion parameters ======\n","    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n","    criterion = nn.CrossEntropyLoss()\n","    #criterion = nn.BCELoss()\n","    if args.load_checkpoint:\n","        checkpoint = torch.load(args.checkpoint_path)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","\n","    # ====== start training the model ======\n","    for epoch in range(args.epochs):\n","        start_epoch = time.time()\n","        train_loss, train_acc = train_model(model, dataloaders['train'], device, optimizer, criterion)\n","\n","        if (epoch % args.checkpoint_interval) == 0:\n","            hp_dict = {'model_state_dict': model.state_dict()}\n","            save_model_dir = os.path.join(folder_dir, 'Saved_model_checkpoints')\n","            create_folder_dir_if_needed(save_model_dir)\n","            torch.save(hp_dict, os.path.join(save_model_dir, 'epoch_{}.pth.tar'.format(epoch)))\n","\n","        if (epoch % args.val_check_interval) == 0:\n","            val_loss, val_acc, predicted_labels, images = test_model(model, dataloaders['val'], device,\n","                                           criterion)\n","            plot_images_with_predicted_labels(images, label_decoder_dict, predicted_labels, folder_dir, epoch)\n","            end_epoch = time.time()\n","            # ====== print the status to the console and write it in tensorboard =======\n","            print('Epoch {} : Train loss {:.8f}, Train acc {:.3f}, Val loss {:.8f}, Val acc {:.3f}, epoch time {:.4f}'\n","                  .format(epoch, train_loss, train_acc, val_loss, val_acc, end_epoch - start_epoch))\n","            tensorboard_writer.add_scalars('train/val loss', {'train_loss': train_loss,\n","                                                              'val loss': val_loss}, epoch)\n","            tensorboard_writer.add_scalars('train/val accuracy', {'train_accuracy': train_acc,\n","                                                                  'val accuracy': val_acc}, epoch)\n","            # ====== save the loss and accuracy in txt file ======\n","            save_loss_info_into_a_file(train_loss, val_loss, train_acc, val_acc, folder_dir, epoch)\n","        \n","\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":20,"outputs":[{"output_type":"stream","text":["The setting of the run are:\n","Namespace(batch_size=2, bidirectional=True, checkpoint_interval=5, checkpoint_path='', epochs=2, hidden_size=256, latent_dim=512, load_all_data_to_RAM=False, load_checkpoint=False, local_dir='/content/drive/My Drive/directed studies/', lr=0.0005, lstm_layers=2, num_workers=4, number_of_classes=2, open_new_folder='True', sampled_data_dir='/content/drive/My Drive/directed studies/testingUCF101_sampled_data_video_sampling_rate_5_num frames extracted_15/', seed=42, split_size=0.2, val_check_interval=5)\n","\n","The training would take place on cuda:0\n","\n","The project directory is /content/drive/My Drive/directed studies/20210113-195346\n","Initializing Datasets and Dataloaders...\n","/content/drive/My Drive/directed studies/testingUCF101_sampled_data_video_sampling_rate_5_num frames extracted_15/ /content/drive/My Drive/directed studies/20210113-195346\n","Videopath ['/content/drive/My Drive/directed studies/testingUCF101_sampled_data_video_sampling_rate_5_num frames extracted_15/1-Smoking0.avi', '/content/drive/My Drive/directed studies/testingUCF101_sampled_data_video_sampling_rate_5_num frames extracted_15/1-Smoking1.avi', '/content/drive/My Drive/directed studies/testingUCF101_sampled_data_video_sampling_rate_5_num frames extracted_15/1-Smoking2.avi', '/content/drive/My Drive/directed studies/testingUCF101_sampled_data_video_sampling_rate_5_num frames extracted_15/1-Smoking13.avi', '/content/drive/My Drive/directed studies/testingUCF101_sampled_data_video_sampling_rate_5_num frames extracted_15/1-Smoking14.avi', '/content/drive/My Drive/directed studies/testingUCF101_sampled_data_video_sampling_rate_5_num frames extracted_15/1-Smoking15.avi', '/content/drive/My Drive/directed studies/testingUCF101_sampled_data_video_sampling_rate_5_num frames extracted_15/1-Smoking16.avi', '/content/drive/My Drive/directed studies/testingUCF101_sampled_data_video_sampling_rate_5_num frames extracted_15/1-Smoking17.avi', '/content/drive/My Drive/directed studies/testingUCF101_sampled_data_video_sampling_rate_5_num frames extracted_15/1-Smoking4.avi', '/content/drive/My Drive/directed studies/testingUCF101_sampled_data_video_sampling_rate_5_num frames extracted_15/1-Smoking5.avi', '/content/drive/My Drive/directed studies/testingUCF101_sampled_data_video_sampling_rate_5_num frames extracted_15/1-Smoking6.avi', '/content/drive/My Drive/directed studies/testingUCF101_sampled_data_video_sampling_rate_5_num frames extracted_15/1-Smoking9.avi', '/content/drive/My Drive/directed studies/testingUCF101_sampled_data_video_sampling_rate_5_num frames extracted_15/1-Smoking8.avi', '/content/drive/My Drive/directed studies/testingUCF101_sampled_data_video_sampling_rate_5_num frames extracted_15/1-Smoking7.avi', '/content/drive/My Drive/directed studies/testingUCF101_sampled_data_video_sampling_rate_5_num frames extracted_15/1-Smoking11.avi', '/content/drive/My Drive/directed studies/testingUCF101_sampled_data_video_sampling_rate_5_num frames extracted_15/1-Smoking10.avi', '/content/drive/My Drive/directed studies/testingUCF101_sampled_data_video_sampling_rate_5_num frames extracted_15/1-Smoking12.avi']\n","hello\n","['1-Smoking0', '1-Smoking1', '1-Smoking2', '1-Smoking13', '1-Smoking14', '1-Smoking15', '1-Smoking16', '1-Smoking17', '1-Smoking4', '1-Smoking5', '1-Smoking6', '1-Smoking9', '1-Smoking8', '1-Smoking7', '1-Smoking11', '1-Smoking10', '1-Smoking12']\n","{1: 'Smoking', 2: 'Not Smoking'}\n","Data prepared\n","Loading model...\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/7 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["1-Smoking16.avi\n","1-Smoking2.avi\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"," 14%|█▍        | 1/7 [00:00<00:05,  1.11it/s]"],"name":"stderr"},{"output_type":"stream","text":["1-Smoking14.avi\n","1-Smoking4.avi\n"],"name":"stdout"},{"output_type":"stream","text":["\r 29%|██▊       | 2/7 [00:01<00:04,  1.13it/s]"],"name":"stderr"},{"output_type":"stream","text":["1-Smoking8.avi\n","1-Smoking12.avi\n"],"name":"stdout"},{"output_type":"stream","text":["\r 43%|████▎     | 3/7 [00:02<00:03,  1.15it/s]"],"name":"stderr"},{"output_type":"stream","text":["1-Smoking17.avi\n","1-Smoking5.avi\n"],"name":"stdout"},{"output_type":"stream","text":["\r 57%|█████▋    | 4/7 [00:03<00:02,  1.14it/s]"],"name":"stderr"},{"output_type":"stream","text":["1-Smoking9.avi\n","1-Smoking7.avi\n"],"name":"stdout"},{"output_type":"stream","text":["\r 71%|███████▏  | 5/7 [00:04<00:01,  1.15it/s]"],"name":"stderr"},{"output_type":"stream","text":["1-Smoking11.avi\n","1-Smoking13.avi\n"],"name":"stdout"},{"output_type":"stream","text":["\r 86%|████████▌ | 6/7 [00:05<00:00,  1.16it/s]"],"name":"stderr"},{"output_type":"stream","text":["1-Smoking6.avi\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:05<00:00,  1.25it/s]\n","  0%|          | 0/2 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["1-Smoking15.avi\n","1-Smoking1.avi\n"],"name":"stdout"},{"output_type":"stream","text":["\r 50%|█████     | 1/2 [00:00<00:00,  1.17it/s]"],"name":"stderr"},{"output_type":"stream","text":["1-Smoking0.avi\n","1-Smoking10.avi\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 2/2 [00:01<00:00,  1.20it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["predicted label 0\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/7 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0 : Train loss 0.41578164, Train acc 100.000, Val loss 0.31389610, Val acc 100.000, epoch time 8.6908\n","1-Smoking16.avi\n","1-Smoking7.avi\n"],"name":"stdout"},{"output_type":"stream","text":["\r 14%|█▍        | 1/7 [00:00<00:05,  1.19it/s]"],"name":"stderr"},{"output_type":"stream","text":["1-Smoking8.avi\n","1-Smoking2.avi\n"],"name":"stdout"},{"output_type":"stream","text":["\r 29%|██▊       | 2/7 [00:01<00:04,  1.19it/s]"],"name":"stderr"},{"output_type":"stream","text":["1-Smoking9.avi\n","1-Smoking14.avi\n"],"name":"stdout"},{"output_type":"stream","text":["\r 43%|████▎     | 3/7 [00:02<00:03,  1.19it/s]"],"name":"stderr"},{"output_type":"stream","text":["1-Smoking6.avi\n","1-Smoking5.avi\n"],"name":"stdout"},{"output_type":"stream","text":["\r 57%|█████▋    | 4/7 [00:03<00:02,  1.18it/s]"],"name":"stderr"},{"output_type":"stream","text":["1-Smoking4.avi\n","1-Smoking13.avi\n"],"name":"stdout"},{"output_type":"stream","text":["\r 71%|███████▏  | 5/7 [00:04<00:01,  1.18it/s]"],"name":"stderr"},{"output_type":"stream","text":["1-Smoking17.avi\n","1-Smoking11.avi\n"],"name":"stdout"},{"output_type":"stream","text":["\r 86%|████████▌ | 6/7 [00:05<00:00,  1.18it/s]"],"name":"stderr"},{"output_type":"stream","text":["1-Smoking12.avi\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 7/7 [00:05<00:00,  1.27it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"mFuPYQ9J4xZw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Heyo0lHEU28Q"},"source":["After successful running of the above code, a folder will be creater which contains the output of the model. The model saves checkpoints, loss per epoch and other details.\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"QUzPJpaSWGrk"},"source":[""],"execution_count":null,"outputs":[]}]}