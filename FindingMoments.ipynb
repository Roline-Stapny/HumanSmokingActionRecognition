{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FindingMoments.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP8JP8IkQLg+1++Zq1z0Jdf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"22c798b1c9844af9bf08351ab159eb3e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0b14120e48354d519caeb78fd93393d4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_935c24cf95ea41c58bc6da4a3073f39c","IPY_MODEL_e223850c5a7b43ab823b67cafa7b3ad2"]}},"0b14120e48354d519caeb78fd93393d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"935c24cf95ea41c58bc6da4a3073f39c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f21fcc18dc2a45b8b8932231f4035930","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":241530880,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":241530880,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_84eafa7a7f1d47038167cdcf2baf5c3f"}},"e223850c5a7b43ab823b67cafa7b3ad2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_39c10a430331457a9f6fbbe59d64948f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 230M/230M [00:05&lt;00:00, 46.8MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cc53055bebf14a5ebfc9e8daf5fbe49b"}},"f21fcc18dc2a45b8b8932231f4035930":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"84eafa7a7f1d47038167cdcf2baf5c3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"39c10a430331457a9f6fbbe59d64948f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"cc53055bebf14a5ebfc9e8daf5fbe49b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"M9O-eX3ewjwX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610585093131,"user_tz":360,"elapsed":30246,"user":{"displayName":"Roline Saldanha","photoUrl":"","userId":"08007127351234095824"}},"outputId":"0194c550-8b75-4e30-ecf3-3bb857ab7541"},"source":["from google.colab import drive\n","drive.flush_and_unmount\n","drive.mount(\"/content/drive/\", force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H4ie-WvAaEka","executionInfo":{"status":"ok","timestamp":1610585096083,"user_tz":360,"elapsed":11272,"user":{"displayName":"Roline Saldanha","photoUrl":"","userId":"08007127351234095824"}},"outputId":"8e36c77e-37fe-4b7f-8b50-ffd841d8d26d"},"source":["!pip install scikit-video"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting scikit-video\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/a6/c69cad508139a342810ae46e946ebb3256aa6e42f690d901bb68f50582e3/scikit_video-1.1.11-py2.py3-none-any.whl (2.3MB)\n","\u001b[K     |████████████████████████████████| 2.3MB 17.0MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from scikit-video) (1.4.1)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from scikit-video) (7.0.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-video) (1.19.5)\n","Installing collected packages: scikit-video\n","Successfully installed scikit-video-1.1.11\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"egzIIO8ngTa2","executionInfo":{"status":"ok","timestamp":1610585105418,"user_tz":360,"elapsed":6780,"user":{"displayName":"Roline Saldanha","photoUrl":"","userId":"08007127351234095824"}}},"source":["import json\r\n","import time\r\n","import cv2\r\n","import matplotlib.pyplot as plt\r\n","import matplotlib.animation as manimation\r\n","import matplotlib.patheffects as pe\r\n","import matplotlib.patches as mpatches\r\n","import torch\r\n","from torch.utils.data import DataLoader, TensorDataset\r\n","import torch.nn.functional as F\r\n","import torchvision.transforms as transforms\r\n","from sklearn.model_selection import train_test_split\r\n","from sklearn.metrics import confusion_matrix\r\n","import math\r\n","from tqdm import tqdm\r\n","from random import sample\r\n","import numpy as np\r\n","from collections import Counter\r\n","import sys\r\n","from PIL import Image\r\n","import glob\r\n","import argparse\r\n","import os\r\n","import torch.nn as nn\r\n","from torchvision import models\r\n","from torch.utils.data import Dataset\r\n","import skvideo\r\n","import skvideo.io\r\n","from torch.utils.data.sampler import Sampler\r\n","import pickle"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"0vvjtLE1wzS-","executionInfo":{"status":"ok","timestamp":1610585112515,"user_tz":360,"elapsed":12818,"user":{"displayName":"Roline Saldanha","photoUrl":"","userId":"08007127351234095824"}}},"source":["#contains all the supporting functions required\n","def set_project_folder_dir(if_open_new_folder, local_dir, use_model_folder_dir=False, mode=None):\n","    if use_model_folder_dir:\n","        folder_dir = os.path.join(os.path.normpath(local_dir + os.sep + os.pardir), mode)\n","        create_folder_dir_if_needed(folder_dir)\n","    else:\n","        if if_open_new_folder != 'False':\n","            folder_dir = open_new_folder(if_open_new_folder, local_dir)\n","        else:\n","            folder_dir = local_dir\n","    return folder_dir\n","\n","\n","def open_new_folder(if_open_new_folder, local_dir):\n","    if if_open_new_folder == 'True':\n","        folder_name = time.strftime(\"%Y%m%d-%H%M%S\")\n","    else:\n","        folder_name = 'debug'\n","    folder_dir = os.path.join(local_dir, folder_name)\n","    create_folder_dir_if_needed(folder_dir)\n","    return folder_dir\n","\n","\n","def save_setting_info(args, device, folder_dir):\n","    setting_file_name = os.path.join(folder_dir, 'setting_info.txt')\n","    args_dict = args.__dict__\n","    with open(setting_file_name, 'w') as f:\n","        for key, value in args_dict.items():\n","            f.write(key + ' : ' + str(value) + '\\n')\n","        f.write(str(device))\n","\n","\n","def plot_label_distribution(dataloaders, folder_dir, load_all_data_to_RAM_mode, label_decoder_dict, mode='train'):\n","    if mode == 'train':\n","        datasets = [dataloaders[dataloader_name].dataset for dataloader_name in dataloaders.keys()]\n","        plot_distribution(datasets, list(dataloaders.keys()), load_all_data_to_RAM_mode, folder_dir, label_decoder_dict)\n","    else:\n","        plot_distribution([dataloaders.dataset], ['test'], load_all_data_to_RAM_mode, folder_dir, label_decoder_dict)\n","\n","\n","def plot_distribution(datasets_list, dataset_names_list, load_all_data_to_RAM_mode, folder_dir, label_decoder_dict):\n","    plt.figure(figsize=(10, 6))\n","    for index, dataset in enumerate(datasets_list):\n","        if load_all_data_to_RAM_mode:\n","            counter_occurrence_of_each_class = Counter(dataset.tensors[1].tolist())\n","        else:\n","            counter_occurrence_of_each_class = Counter(dataset.labels)\n","        with open(os.path.join(folder_dir, 'frequency_of_each_class_{}.pkl'.format(dataset_names_list[index])), 'wb') as f:\n","            pickle.dump(counter_occurrence_of_each_class, f, pickle.HIGHEST_PROTOCOL)\n","        sorted_counter = sorted(counter_occurrence_of_each_class.items())\n","        x, y = zip(*sorted_counter)\n","        plt.bar(x, y)\n","    plt.legend(dataset_names_list)\n","    plt.title('The frequency of each class\\n' + '&'.join(dataset_names_list))\n","    plt.xlabel('label')\n","    plt.ylabel('Frequency')\n","    x_ticks_labels = [label_decoder_dict[label_code] for label_code in x]\n","    plt.xticks(x, x_ticks_labels, fontsize=8, rotation=90)\n","    plt.yticks(fontsize=8)\n","    plt.tight_layout()\n","    plt.xlim(-1, max(x) + 1)\n","    plt.savefig(os.path.join(folder_dir, '_'.join(dataset_names_list) + '.jpg'), dpi=300, bbox_inches=\"tight\")\n","    plt.close()\n","\n","\n","\n","def split_data(ucf_list_root, seed, number_of_classes, split_size, folder_dir):\n","    print(ucf_list_root, folder_dir)\n","    video_names_train, video_names_test, labels, labels_decoder_dict = get_video_list(ucf_list_root, number_of_classes, folder_dir)\n","    print(\"hello\")\n","    print(video_names_train)\n","    video_names_train, video_names_val, labels_train, labels_val = train_test_split(video_names_train, labels,\n","                                                                                    test_size=split_size,\n","                                                                                    random_state=seed)\n","    \n","    save_video_names_test_and_add_labels(video_names_test, labels_decoder_dict, folder_dir, number_of_classes)\n","    # save labels_decoder_dict\n","    with open(os.path.join(folder_dir, 'labels_decoder_dict.pkl'), 'wb') as f:\n","        pickle.dump(labels_decoder_dict, f, pickle.HIGHEST_PROTOCOL)\n","    return [video_names_train, labels_train], [video_names_val, labels_val], labels_decoder_dict\n","\n","\n","def get_data(mode, video_names, list, number_of_classes, labels=[]):\n","    # setting the data files as a list so the not overpower the system\n","    for video_name in video_names:\n","        if mode == 'train':\n","            label = video_name.split('-')[0]\n","            video_name =  video_name\n","\n","            label = int(label.rstrip('\\n'))\n","            if number_of_classes is None or label in range(1, number_of_classes + 1):\n","                labels.append(label-1)\n","                list.append(video_name.split('.')[0])\n","            else:\n","                continue\n","        else:\n","            list.append(video_name.split('.')[0])\n","    return list, labels\n","\n","\n","def get_video_list(ucf_list_root, number_of_classes, folder_dir):\n","    # ====== get a list of video names ======\n","    video_names_train, video_names_test, labels = [], [], []\n","    sample_train_test_split = str(sample(range(1, 4), 1)[0])\n","    with open(os.path.join(folder_dir, 'setting_info.txt'), 'a+') as f:\n","        f.write('\\nThe test/train split that we have train on is {}'.format(sample_train_test_split))\n","\n","      \n","        videopath = glob.glob(ucf_list_root+\"*.avi\")\n","        print(\"Videopath\", videopath)\n","        video_names = []\n","        for path in videopath:\n","          video_names.append(path.split('/')[-1])\n","\n","\n","        \n","        video_names_train, labels = get_data('train', video_names, video_names_train, number_of_classes, labels)\n","        labels_decoder_dict = {1 : \"Smoking\", 2 : \"Not Smoking\"}\n","\n","        video_names_test, _ = get_data('test', video_names, video_names_test, number_of_classes)\n","\n","    return video_names_train, video_names_test, labels, labels_decoder_dict\n","\n","\n","def save_video_names_test_and_add_labels(video_names_test, labels_decoder_dict, folder_dir, number_of_classes):\n","    save_test_video_details = os.path.join(folder_dir, 'test_videos_detailes.txt')\n","    with open(save_test_video_details, 'w') as f:\n","        for text_video_name in video_names_test:\n","            label_string = text_video_name.split('/')[0]\n","            # endoce label\n","            for key, value in labels_decoder_dict.items():\n","                if value == label_string:\n","                    label_code = key\n","                else:\n","                    continue\n","                if number_of_classes is None or label_code in range(0, number_of_classes):\n","                    f.write(text_video_name + ' ' + str(label_code) + '\\n')\n","                else:\n","                    continue\n","\n","\n","def plot_images_with_predicted_labels(local_x, label_decoder_dict, predicted_labels, folder_dir, epoch):\n","    print(\"labl Dic\")\n","    print(label_decoder_dict)\n","    print(\"PredictedLabels\")\n","    print(predicted_labels)\n","    folder_save_images = os.path.join(folder_dir, 'Images')\n","    create_folder_dir_if_needed(folder_save_images)\n","    n_rows = math.trunc(math.sqrt(len(local_x)))\n","    n_cols = n_rows\n","    if n_rows == 1 and n_cols == 1:\n","        plot_single_images_with_predicted_labels(local_x, label_decoder_dict, predicted_labels, folder_save_images, epoch)\n","    else:\n","        fig, ax = plt.subplots(ncols=n_cols, nrows=n_rows, figsize=(10, 10))\n","        for row in range(n_rows):\n","            for col in range(n_cols):\n","                img = local_x[col + (row * n_cols)][0].permute(1, 2, 0)\n","                img_scale = (img - img.min()) / (img.max() - img.min())\n","                ax[row, col].imshow(img_scale)\n","                label_for_title = label_decoder_dict[predicted_labels[col + (row * n_cols)].item()]\n","                #label_for_title = str(predicted_labels[col + (row * n_cols)].item())\n","                ax[row, col].set_title(label_for_title)\n","                ax[row, col].set_xticks([])\n","                ax[row, col].set_yticks([])\n","        plt.savefig(os.path.join(folder_save_images, 'predicted_labels {} epoch.png'.format(epoch)))\n","        plt.close()\n","\n","\n","def plot_single_images_with_predicted_labels(local_x, label_decoder_dict, predicted_labels, folder_save_images, epoch):\n","    fig, ax = plt.subplots(figsize=(10, 10))\n","    img = local_x[0][0].permute(1, 2, 0)\n","    img_scale = (img - img.min()) / (img.max() - img.min())\n","    ax.imshow(img_scale)\n","    label_for_title = label_decoder_dict[predicted_labels[0].item()]\n","    ax.set_title(label_for_title)\n","    ax.set_xticks([])\n","    ax.set_yticks([])\n","    plt.savefig(os.path.join(folder_save_images, 'predicted_labels {} epoch.png'.format(epoch)))\n","    plt.close()\n","\n","\n","def create_folder_dir_if_needed(folder_save_dir):\n","    if not os.path.exists(folder_save_dir):\n","        os.makedirs(folder_save_dir)\n","\n","\n","\n","def load_all_dataset_to_RAM(dataloaders, dataset_order, batch_size):\n","    images_train, labels_train, images_val, labels_val = [], [], [], []\n","    for i, mode in enumerate(['train', 'val']):\n","        images_list = [images_train, images_val][i]\n","        labels_list = [labels_train, labels_val][i]\n","        with tqdm(total=len(dataloaders[mode])) as pbar:\n","            # with tqdm_notebook(total=len(dataloaders[mode])) as pbar:\n","            for local_images, local_label in dataloaders[mode]:\n","                images_list += [local_images]\n","                labels_list += [local_label]\n","                pbar.update(1)\n","    images_train = torch.cat(images_train, axis=0)\n","    labels_train = torch.cat(labels_train, axis=0)\n","    images_val = torch.cat(images_val, axis=0)\n","    labels_val = torch.cat(labels_val, axis=0)\n","    datasets = {dataset_order[index]: TensorDataset(x[0], x[1]) for index, x in\n","                enumerate([[images_train, labels_train], [images_val, labels_val]])}\n","    dataloaders = {x: DataLoader(datasets[x], batch_size=batch_size, shuffle=True)\n","                   for x in ['train', 'val']}\n","    return dataloaders\n","\n","\n","def load_all_dataset_to_RAM_test(dataloader, batch_size):\n","    images_test, labels_test = [], []\n","    with tqdm(total=len(dataloader)) as pbar:\n","        \n","        for local_images, local_label in dataloader:\n","            images_test += [local_images]\n","            labels_test += [local_label]\n","            pbar.update(1)\n","    images_test = torch.cat(images_test, axis=0)\n","    labels_test = torch.cat(labels_test, axis=0)\n","    dataset = TensorDataset(images_test, labels_test)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","    return dataloader\n","\n","\n","def foward_step_no_labels(model, images):\n","    # Must be done before you run a new batch. Otherwise the LSTM will treat a new batch as a continuation of a sequence\n","    model.Lstm.reset_hidden_state()\n","    with torch.no_grad():\n","        output = model(images)\n","    predicted_labels = output.detach().cpu().argmax(dim=1)\n","    return predicted_labels\n","\n","\n","def foward_step(model, images, labels, criterion, mode=''):  # predections\n","    # Must be done before you run a new batch. Otherwise the LSTM will treat a new batch as a continuation of a sequence\n","    model.Lstm.reset_hidden_state()\n","    if mode == 'test':\n","        with torch.no_grad():\n","            output = model(images)\n","    else:\n","        output = model(images)\n","    loss = criterion(output, labels)\n","    # Accuracy calculation\n","    predicted_labels = output.detach().argmax(dim=1)\n","    acc = (predicted_labels == labels).cpu().numpy().sum()\n","    return loss, acc, predicted_labels.cpu()\n","\n","\n","def train_model(model, dataloader, device, optimizer, criterion):\n","    train_loss, train_acc = 0.0, 0.0\n","    model.train()\n","    with tqdm(total=len(dataloader)) as pbar:\n","        # with tqdm_notebook(total=len(dataloader)) as pbar:\n","        for local_images, local_labels, ___ in dataloader:\n","            local_images, local_labels = local_images.to(device), local_labels.to(device)\n","            optimizer.zero_grad()  # zero the parameter gradients\n","            loss, acc, ___ = foward_step(model, local_images, local_labels, criterion, mode='train')\n","            train_loss += loss.item()\n","            train_acc += acc\n","            loss.backward()  # compute the gradients\n","            optimizer.step()  # update the parameters with the gradients\n","            pbar.update(1)\n","    train_acc = 100 * (train_acc / dataloader.dataset.__len__())\n","    train_loss = train_loss / len(dataloader)\n","    return train_loss, train_acc\n","\n","\n","def test_model(model, dataloader, device, criterion, mode='test'):\n","    val_loss, val_acc = 0.0, 0.0\n","    model.eval()\n","    if mode == 'save_prediction_label_list':\n","        prediction_labels_list = []\n","        true_labels_list = []\n","    with tqdm(total=len(dataloader)) as pbar:\n","        # with tqdm_notebook(total=len(dataloader)) as pbar:\n","        for local_images, local_labels, indexs in dataloader:\n","                local_images, local_labels = local_images.to(device), local_labels.to(device)\n","                loss, acc, predicted_labels = foward_step(model, local_images, local_labels, criterion, mode='test')\n","                if mode == 'save_prediction_label_list':\n","                    prediction_labels_list += [predicted_labels.detach().cpu()]\n","                    true_labels_list += [local_labels.detach().cpu()]\n","                val_loss += loss.item()\n","                val_acc += acc\n","                pbar.update(1)\n","    val_acc = 100 * (val_acc / dataloader.dataset.__len__())\n","    val_loss = val_loss / len(dataloader)\n","    if mode == 'save_prediction_label_list':\n","        return val_loss, val_acc, prediction_labels_list, local_images.cpu(), true_labels_list, indexs\n","    else:\n","        return val_loss, val_acc, predicted_labels, local_images.cpu()\n","\n","\n","def test_model_continues_movie(model, dataloader, device, criterion, save_path, label_decoder_dict):\n","    val_loss, val_acc = 0.0, 0.0\n","    model.eval()\n","    # ====== choosing one random batch from the dataloader ======\n","    dataloader_iter = iter(dataloader)\n","    images, labels, ___ = next(dataloader_iter)\n","    predicted_labels_list = []\n","    # ===== create continues movie and labels tensor, with X frames from each movie ======\n","    # ===== and stack a sliding window of size 5 frames to new dim so they will act as batch ======\n","    num_frames_to_sample = images.shape[1]\n","    sliding_window_images, continues_labels, continues_movie = create_sliding_window_x_frames_size_dataset\\\n","        (images, labels, num_frames_to_sample)\n","    # ====== predict the label of each sliding window, use batches because of GPU memory ======\n","    for batch_boundaries in range(0, len(sliding_window_images), dataloader.batch_size):\n","        batch_images_to_plot = sliding_window_images[batch_boundaries: batch_boundaries + dataloader.batch_size].to(\n","            device)\n","        batch_labels = continues_labels[batch_boundaries: batch_boundaries + dataloader.batch_size].to(device)\n","        loss, acc, predicted_labels = foward_step(model, batch_images_to_plot, batch_labels, criterion, mode='test')\n","        predicted_labels_list += [predicted_labels]\n","        val_acc += acc\n","    predicted_labels = torch.cat(predicted_labels_list, axis=0)\n","    val_loss += loss.item()\n","    create_video_with_labels(save_path, 'Video_with_prediction_vs_true_labels.avi', continues_movie, continues_labels,\n","                             predicted_labels, label_decoder_dict, mode='continues_test_movie')\n","    save_path_plots = os.path.join(save_path, 'Plots')\n","    create_folder_dir_if_needed(save_path_plots)\n","    plot_sliding_window_prediction_for_each_frame(continues_labels, predicted_labels, save_path_plots,\n","                                                  label_decoder_dict, labels)\n","    plot_function_of_num_frames_in_window_on_prediction(continues_labels, predicted_labels, save_path_plots,\n","                                                        num_frames_to_sample)\n","    val_acc = 100 * (val_acc / len(sliding_window_images))\n","    val_loss = val_loss / len(dataloader)\n","    return val_loss, val_acc, predicted_labels, images.cpu()\n","\n","\n","def test_model_continues_movie_youtube(model, data, device, save_path, label_decoder_dict, batch_size,\n","                                       preprocessing_movie_mode, dataset_type='youtube', video_original_size=None, vid=None, pth=None):\n","    model.eval()\n","    if preprocessing_movie_mode == 'preprocessed':\n","        # ====== choosing one random batch from the dataloader ======\n","        dataloader_iter = iter(data)\n","        images = next(dataloader_iter)\n","        images = images.squeeze(0)\n","        video_original_size = video_original_size[dataloader_iter._dataset.images[0].split('.avi')[0]]\n","    else:\n","        images = data\n","\n","    num_frames_to_sample = 5\n","    cfps = 10\n","\n","    # ===== create continues movie and labels tensor, with X frames from each movie ======\n","    # ===== and stack a sliding window of size 5 frames to new dim so they will act as batch ======\n","\n","    sliding_window_images = create_sliding_window_x_frames_size_dataset \\\n","        (images, None, num_frames_to_sample, dataset_type)\n","\n","    # ====== predict the label of each sliding window, use batches beacuse of GPU memory ======\n","\n","    predicted_labels = predict_labels_of_sliding_window(sliding_window_images, batch_size, device, model)\n","\n","    #f_labels = [1 for _ in range(num_frames_to_sample - 1)] + predicted_labels\n","\n","    print(\"Len of predicted labels\", len(predicted_labels))\n","    model_prediction= predicted_labels.tolist()\n","    f_labels = [1 for _ in range(num_frames_to_sample - 1)] + model_prediction + [1]\n","    print(\"f_labelslen\", len(f_labels))\n","    print(f_labels)\n","    video = cv2.VideoCapture(os.path.join(pth, vid))\n","    fps = video.get(cv2.CAP_PROP_FPS)\n","\n","    start = 0\n","    end = 0\n","    flag = False\n","    moments=[]\n","    moments_in_seconds = []\n","    for i in range(len(f_labels)):\n","      if f_labels[i] == 0 and not flag:\n","        start = i\n","        flag = True\n","      if flag and f_labels[i] == 1:\n","        moments.append([start,i-1])\n","        s, e = moments[-1]\n","        moments_in_seconds.append([(s/fps), (e/fps)]) \n","        flag = False\n","\n","    print(moments)\n","    print(moments_in_seconds)\n","\n","    save_path_plots = os.path.join(save_path, 'Plots')\n","    create_folder_dir_if_needed(save_path_plots)\n","    plot_sliding_window_prediction_for_each_frame_no_labels(predicted_labels, save_path_plots, label_decoder_dict)\n","    # create_video_with_labels(save_path, 'Video_with_prediction_vs_true_labels.avi',\n","    #                          images[:len(images) - num_frames_to_sample + 1], None, predicted_labels,\n","    #                          label_decoder_dict,\n","    #                          video_original_size=video_original_size, fps=cfps, mode='youtube')\n","    return moments_in_seconds\n","\n","def create_sliding_window_x_frames_size_dataset(local_images, local_labels, num_frames_to_sample,\n","                                                dataset_type='UCF101'):\n","    \"\"\"\"\n","    This function would join all of the images in the batch to one long continues movie, which would be\n","    composed from num_batch human action movies (shape - num_batch*num_frames_to_sample, 3, 224, 224).\n","    Than, a sliding window of num_frames_to_sample would be passed on the continues movie,\n","    creating a stack of mini videos that can be used as an input to the LRCN network.\n","    (shape - (num_batch - num_frames_to_sample+1), num_of_frames_to_samples, 3, 224, 224)\n","    The label for each sliding window would be set according the majority of frames we have for each action,\n","    meaning if the sliding window has 3 frames from the first action and two from the next action, the label of the sliding\n","    window would be the first action\n","    \"\"\"\n","    # ===== create continues movie, with X frames from each movie ======\n","    if dataset_type == 'UCF101':\n","        local_images = local_images[:, :num_frames_to_sample]\n","        continues_frames = local_images.view(local_images.shape[0] * local_images.shape[1], local_images.shape[2],\n","                                             local_images.shape[3], local_images.shape[4])\n","    else:\n","        continues_frames = local_images\n","    sliding_window_images = []\n","    \n","    for num_frame in range(continues_frames.shape[0] - num_frames_to_sample + 1):\n","        # ===== normalize the frames according to the imagenet preprocessing =======\n","        sliding_window_images += [continues_frames[num_frame: num_frame + num_frames_to_sample]]\n","    sliding_window_images = torch.stack(sliding_window_images)\n","    continues_frames = continues_frames[:len(sliding_window_images)]\n","    if dataset_type == 'UCF101':\n","        # ==== create continues label tensor where each frame has its own label ======\n","        majority_of_num_of_frames = math.ceil(\n","            num_frames_to_sample / 2) if num_frames_to_sample % 2 != 0 else num_frames_to_sample / 2 + 1\n","        majority_of_num_of_frames = int(majority_of_num_of_frames)\n","        print(\"Majority_of_num_frames \",majority_of_num_of_frames)\n","        mid_continues_labels = local_labels[1:len(local_labels) - 1].view(-1, 1).repeat(1, num_frames_to_sample).view(\n","            -1)\n","        start_continues_labels = local_labels[0].view(-1, 1).repeat(1, majority_of_num_of_frames).view(-1)\n","        end_continues_labeels = local_labels[-1].view(-1, 1).repeat(1, majority_of_num_of_frames).view(-1)\n","        continues_labels = torch.cat((start_continues_labels, mid_continues_labels, end_continues_labeels))\n","        return sliding_window_images, continues_labels, continues_frames\n","    else:\n","        return sliding_window_images\n","\n","\n","def plot_function_of_num_frames_in_window_on_prediction(continues_labels, predicted_labels, save_path_plots,\n","                                                        num_frames_to_sample):\n","    mean_acc_array = []\n","    for num_frames in range(num_frames_to_sample):\n","        predicted_labels_with_num_frames_in_window = np.array(\n","            [predicted_labels[i] for i in range(num_frames, len(predicted_labels), num_frames_to_sample)])\n","        labels_with_num_frames = np.array(\n","            [continues_labels[i] for i in range(num_frames, len(continues_labels), num_frames_to_sample)])\n","        print(\"hello\")\n","        print(predicted_labels_with_num_frames_in_window == labels_with_num_frames)\n","\n","        \n","        mean_acc_array += [0 / len(\n","            labels_with_num_frames) * 100]\n","    mean_acc_array.reverse()\n","    x_axis = np.arange(num_frames_to_sample)\n","    plt.plot(x_axis, mean_acc_array, linestyle='-', marker=\"o\")\n","    plt.xticks(x_axis, np.arange(num_frames_to_sample, 0, -1))\n","    plt.xlabel('Number of frames from a specific human action')\n","    plt.ylabel('Mean accuracy [%]')\n","    plt.ylim(0, 100)\n","    plt.title('Change in accuracy with the change in frame num')\n","    plt.savefig(os.path.join(save_path_plots, 'analysis_of_predicted_labels_in_sliding_window.png'), dpi=300,\n","                bbox_inches='tight')\n","    plt.close()\n","\n","\n","def save_loss_info_into_a_file(train_loss, val_loss, train_acc, val_acc, folder_dir, epoch):\n","    file_name = os.path.join(folder_dir, 'loss_per_epoch.txt')\n","    with open(file_name, 'a+') as f:\n","        f.write('Epoch {} : Train loss {:.8f}, Train acc {:.4f}, Val loss {:.8f}, Val acc {:.4f}\\n'\n","                .format(epoch, train_loss, train_acc, val_loss, val_acc))\n","\n","\n","def set_transforms(mode):\n","    if mode == 'train':\n","        transform = transforms.Compose(\n","            [transforms.Resize(256),  # this is set only because we are using Imagenet pre-train model.\n","             transforms.RandomCrop(224),\n","             transforms.RandomHorizontalFlip(),\n","             transforms.ToTensor(),\n","             transforms.Normalize(mean=(0.485, 0.456, 0.406),\n","                                  std=(0.229, 0.224, 0.225))\n","             ])\n","    elif mode == 'test' or mode == 'val':\n","        transform = transforms.Compose([transforms.Resize((224, 224)),\n","                                        transforms.ToTensor(),\n","                                        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n","                                                             std=(0.229, 0.224, 0.225))])\n","    return transform\n","\n","\n","def create_new_video(save_path, video_name, image_array):\n","    \n","    (h, w) = image_array[0].shape[:2]\n","    if len(video_name.split('/')) > 1:\n","        video_name = video_name.split('/')[1]\n","    else:\n","        video_name = video_name.split('.mp4')[0]\n","        video_name = video_name + '.avi'\n","    save_video_path = os.path.join(save_path, video_name)\n","    output_video = cv2.VideoWriter(save_video_path, cv2.VideoWriter_fourcc(*'MJPG'), 5, (w, h), True)\n","    for frame in range(len(image_array)):\n","        output_video.write(image_array[frame])\n","    output_video.release()\n","    cv2.destroyAllWindows()\n","\n","\n","def create_video_with_labels(save_path, video_name, image_array, continues_labels, predicted_labels, label_decoder_dict,\n","                             video_original_size=None, fps=2.5, mode='single_movie'):\n","    if mode == 'single_movie':\n","        predicted_labels = torch.tensor(predicted_labels)\n","        predicted_labels = predicted_labels.view(-1, 1).repeat(1, len(image_array)).view(-1)\n","    path_save_videos = os.path.join(save_path, 'Videos')\n","    create_folder_dir_if_needed(path_save_videos)\n","    dpi = 300\n","    w, h = setting_video_size(video_original_size)\n","    image_array = F.interpolate(image_array, size=(h, w))\n","    image_array = image_array.transpose(2, 1).transpose(2, 3)\n","    n_frames = len(image_array)\n","    figure_size_w = round((w - 50) / float(dpi) * 2)\n","    figure_size_h = round(h / float(dpi) * 3)\n","    h_fig = plt.figure(figsize=(figure_size_w, figure_size_h), dpi=dpi)\n","    # ====== plot frame, would change with every frame ======\n","    if mode != 'youtube':\n","        h_ax = h_fig.add_axes([0.08, 0.25, 0.85, 0.8])\n","    else:\n","        h_ax = h_fig.add_axes([0.03, 0.1, 0.95, 0.95])\n","    img = (image_array[0] - image_array[0].min()) / (image_array[0].max() - image_array[0].min())\n","    h_im = h_ax.matshow(img)\n","    h_ax.set_axis_off()\n","    h_im.set_interpolation('none')\n","    h_ax.set_aspect('equal')\n","    # ======== plot the label prediction with the frame =====\n","    if mode != 'youtube':\n","        h_ax_plot = h_fig.add_axes([0.08, 0.25, 0.85, 0.05])\n","    else:\n","        h_ax_plot = h_fig.add_axes([0.03, 0.25, 0.95, 0.04])\n","    # h_ax_plot = h_fig.add_axes([0.1, 0.22, 0.8, 0.06])\n","    x_array = np.arange(len(predicted_labels)) + 0.5\n","    y_array = np.zeros(len(x_array))\n","    bool_array = None if continues_labels is None else continues_labels == predicted_labels\n","    color_dict = create_color_dict(predicted_labels)\n","    color_list = []\n","    h_text_object = set_text_to_video_frame(continues_labels, label_decoder_dict,\n","                                            predicted_labels, mode, bool_array=bool_array)\n","\n","    FFMpegWriter = manimation.writers['ffmpeg']\n","    metadata = dict(title=video_name, artist='Matplotlib')\n","    writer = FFMpegWriter(fps=fps, metadata=metadata)\n","    with writer.saving(h_fig, os.path.join(path_save_videos, video_name), dpi=dpi):  # change from 600 dpi\n","        for i in range(n_frames):\n","            set_text_to_video_frame(continues_labels, label_decoder_dict,\n","                                    predicted_labels, mode, h_text_object=h_text_object, bool_array=bool_array, frame=i)\n","            img = (image_array[i] - image_array[i].min()) / (image_array[i].max() - image_array[i].min())\n","            h_im.set_array(img)\n","            if i > 0:\n","                h_im_2.remove()\n","            y_array[:i + 1] = 1\n","            if mode != 'continues_test_movie':\n","                color_list += [color_dict[predicted_labels[i].item()]]\n","            else:\n","                color_list += ['green' if bool_array[i].item() else color_dict[predicted_labels[i].item()]]\n","            h_im_2 = h_ax_plot.bar(x_array, y_array, color=color_list, width=1.0)\n","            h_ax_plot.get_yaxis().set_ticks([])\n","            h_ax_plot.set_ylim(0, 1)\n","            h_ax_plot.tick_params(axis=\"x\", labelsize=4)\n","            h_ax_plot.set_xlim(0, len(x_array))\n","            writer.grab_frame()\n","    plt.close()\n","\n","\n","def setting_sample_rate(num_frames_to_extract, sampling_rate, video, fps, ucf101_fps):\n","    video.set(cv2.CAP_PROP_POS_AVI_RATIO, 1)\n","    # video_length = video.get(cv2.CAP_PROP_POS_MSEC) / 1000\n","    # print(\"video_lenght\", video_length, fps )\n","    # num_frames = int(video_length * fps)\n","\n","    # print(\"Total frames: \", num_frames)\n","    \n","    # video_length = video.get(cv2.CAP_PROP_POS_MSEC)\n","    # print(\"VideoLenght\", video_length)\n","\n","    # cal_fps = video.get(cv2.CAP_PROP_FPS)\n","\n","    # print(\"FPS :\",cal_fps)\n","    \n","    num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n","    # print( num_frames )\n","\n","    # num_frames = int(video_length * fps)\n","    print(\"number of frames\", num_frames)\n","\n","    if num_frames_to_extract == 'all':\n","        sample_start_point = 0\n","        if fps != ucf101_fps and sampling_rate != 0:\n","            sampling_rate = math.ceil(fps / (ucf101_fps / sampling_rate))\n","    elif video_length < (num_frames_to_extract * sampling_rate):\n","        print(\"hellp vp\")\n","        sample_start_point = 0\n","        sampling_rate = 2\n","    else:\n","        sample_start_point = sample(range(num_frames - (num_frames_to_extract * sampling_rate)), 1)[0]\n","    return sample_start_point, sampling_rate, num_frames\n","\n","\n","def capture_and_sample_video(row_data_dir, video_name, num_frames_to_extract, sampling_rate, fps, save_path,\n","                             ucf101_fps, processing_mode):\n","    video = cv2.VideoCapture(os.path.join(row_data_dir, video_name))\n","    \n","    if fps == 'Not known':\n","        fps = video.get(cv2.CAP_PROP_FPS)\n","    video_width = video.get(cv2.CAP_PROP_FRAME_WIDTH)\n","    video_height = video.get(cv2.CAP_PROP_FRAME_HEIGHT)\n","    \n","\n","    # video_length = video.get(cv2.CAP_PROP_POS_MSEC)\n","    # print(\"VideoLength\", video_length)\n","\n","    fps = 30\n","    \n","    #video.get(cv2.CAP_PROP_FPS)\n","    ucf101_fps = video.get(cv2.CAP_PROP_FPS)\n","\n","    print(\"FPS :\",ucf101_fps)\n","    \n","    num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n","    print(\"total number of frames\",num_frames)\n","    sample_start_point = 0\n","\n","    # sample_start_point, sampling_rate, num_frames = setting_sample_rate(num_frames_to_extract, sampling_rate, video,\n","    #                                                                      fps, ucf101_fps)\n","    print(\"Details: \", sample_start_point, sampling_rate, num_frames)\n","    # ====== setting the video to start reading from the frame we want ======\n","    image_array = []\n","    if num_frames_to_extract == 'all':\n","        num_frames_to_extract = int(num_frames / sampling_rate) if sampling_rate != 0 else num_frames\n","    \n","    print(\"frames to extract\",num_frames_to_extract)\n","\n","    if processing_mode == 'live':\n","        transform = set_transforms(mode='test')\n","    for frame in range(num_frames_to_extract):\n","        video.set(1, sample_start_point)\n","        success, image = video.read()\n","        if not success:\n","            print('Error in reading frames from row video')\n","        else:\n","            RGB_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) if processing_mode == 'live' else image\n","            image = Image.fromarray(RGB_img.astype('uint8'), 'RGB')\n","            if processing_mode == 'live':\n","                image_array += [transform(image)]\n","            else:\n","                image_array += [np.uint8(image)]\n","        sample_start_point = sample_start_point + sampling_rate\n","    video.release()\n","    if processing_mode == 'main':\n","        create_new_video(save_path, video_name, image_array)\n","    \n","    return image_array, [video_width, video_height]\n","\n","\n","def load_test_data(test_dir, mode='load_all'):\n","\n","\n","    videopath = glob.glob(test_dir+\"*.avi\")\n","\n","    print(\"Videopath\", videopath)\n","\n","    video_names = []\n","    for path in videopath:\n","        print(path.split('/')[-1])\n","        video_names.append(path.split('/')[-1].split(\".\")[0])\n","\n","    if mode == 'load_all':\n","        test_videos_names, labels = [], []\n","        for video_name_with_label in video_names:\n","            label = video_name_with_label.split('-')[0]\n","            test_videos_names += [video_name_with_label]\n","            labels += [int(label) - 1]\n","\n","    labels_decoder_dict = {0 : \"Smoking\", 1 : \"Not Smoking\"}\n","    if mode == 'load_all':\n","        return test_videos_names, labels, labels_decoder_dict\n","    else:\n","        return labels_decoder_dict\n","\n","  \n","\n","def plot_confusion_matrix(predicted_labels, true_labels, label_decoder_dict, save_path):\n","    class_order_to_plot = list(label_decoder_dict.keys())[:true_labels.max() + 1]\n","    cm = confusion_matrix(true_labels, predicted_labels, labels=class_order_to_plot, normalize='true')\n","    # ==== plot the cm as heatmap ======\n","    plt.figure(figsize=(8, 6))\n","    plt.imshow(cm, interpolation='none', aspect='auto', cmap=plt.cm.Blues)\n","    cb = plt.colorbar()\n","    cb.ax.tick_params(labelsize=10)\n","    x_labels = [label_decoder_dict[label_code] for label_code in class_order_to_plot]\n","    plt.xticks(class_order_to_plot, x_labels, rotation=90, fontsize=6)\n","    plt.yticks(class_order_to_plot, x_labels, fontsize=6)\n","    plt.ylim(len(class_order_to_plot), -0.5)\n","    plt.title('Normalized confusion matrix')\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(save_path, 'Normalized_confusion_matrix.png'), dpi=300, bbox_inches='tight')\n","    plt.close()\n","\n","\n","def plot_acc_per_class(predicted_labels, true_labels, label_decoder_dict, save_path):\n","    # ===== count the number of times each class appear in the test data =====\n","    frequency_of_each_class = Counter(true_labels.tolist())\n","    # ===== load the frequency counter for the train dataset, would be used to mark low frequency classes =====\n","    global_dir = os.path.normpath(save_path + os.sep + os.pardir + os.sep + os.pardir)\n","    with open(os.path.join(global_dir, 'frequency_of_each_class_train.pkl'), 'rb') as f:\n","        frequency_of_each_class_train = pickle.load(f)\n","    # ===== count the number of times each class is labeled correctly =======\n","    class_list = list(label_decoder_dict.keys())[: true_labels.max() + 1]\n","    acc = true_labels == predicted_labels\n","    counter_correct_labeled = Counter()\n","    for index, true_label in enumerate(true_labels):\n","        counter_correct_labeled[true_label.item()] += acc[index].item()\n","    # ==== calculate the accuracy to predict each class =====\n","    acc_per_class = []\n","    mean_frequency = sum(list(frequency_of_each_class_train.values())) / len(frequency_of_each_class_train)\n","    classes_with_lower_frequency_compare_to_average = []\n","    for class_ in class_list:\n","        acc_per_class += [counter_correct_labeled[class_] / frequency_of_each_class[class_] * 100]\n","        if frequency_of_each_class_train[class_] <= (0.9 * mean_frequency):\n","            classes_with_lower_frequency_compare_to_average += [class_]\n","    acc_classes_with_lower_frequency_compare_to_average = [acc_per_class[class_] for class_ in\n","                                                           classes_with_lower_frequency_compare_to_average]\n","    plt.figure(figsize=(10, 10))\n","    plt.bar(class_list, acc_per_class)\n","    plt.bar(classes_with_lower_frequency_compare_to_average, acc_classes_with_lower_frequency_compare_to_average,\n","            color='red')\n","    x_labels = [label_decoder_dict[label_code] for label_code in class_list]\n","    plt.xticks(class_list, x_labels, rotation=90, fontsize=12)\n","    plt.yticks(fontsize=12)\n","    plt.xlabel('Classes', fontsize=16)\n","    plt.ylabel('Accuracy [%]', fontsize=16)\n","    plt.xlim(-1, class_list[-1] + 1)\n","    plt.ylim(0, 109)\n","    plt.legend(['freq > 0.9 * avr freq of a class', 'freq <= 0.9 * avr freq of a class'])\n","    plt.title('The accuracy score for each class', fontsize=18)\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(save_path, 'The_accuracy_score_for_each_class.png'), dpi=300, bbox_inches='tight')\n","    plt.close()\n","\n","\n","def check_if_batch_size_bigger_than_num_classes(batch_size, num_of_classes):\n","    if num_of_classes is None:\n","        num_of_classes = 101\n","    if batch_size > num_of_classes:\n","        print(\n","            'Your batch size is bigger than the num of classes you are testing. This would cause an Error in the custom sampler. Your options are:\\n'\n","            '1. Reduce the batch size so it would be smaller or equal to the number of classes you are testing.\\n'\n","            '2. Reduce the number of classes so it would be bigger or equal to the batch size.\\n'\n","            '3. Stop using the custom sampler: erase the sampler parameter from the dataloader and change the shuffle '\n","            'parameter to True.')\n","        sys.exit()\n","\n","\n","def plot_sliding_window_prediction_for_each_frame(continues_labels, predicted_labels, save_path_plots,\n","                                                  label_decoder_dict, original_order_of_labels):\n","  \n","    max_label_code = max(max(predicted_labels).item(), max(continues_labels).item())\n","    print(\"MAx label code\")\n","    print(max_label_code)\n","\n","    predicted_labels_one_hot = create_one_hot_vector_matrix(predicted_labels.numpy(), max_label_code)\n","\n","    labels_one_hot = create_one_hot_vector_matrix(continues_labels.numpy(), max_label_code)\n","    labels_one_hot = labels_one_hot * 2\n","    one_hot_matrix_to_plot = predicted_labels_one_hot + labels_one_hot\n","    labels_new_order = original_order_of_labels.cpu().detach().numpy()\n","    #one_hot_matrix_to_plot, labels_new_order = resort_matrix(original_order_of_labels, one_hot_matrix_to_plot)\n","    one_hot_matrix_to_plot = one_hot_matrix_to_plot[~np.all(one_hot_matrix_to_plot == 0, axis=1)]\n","    one_hot_matrix_to_plot = np.apply_along_axis(increase_the_error_value_for_non_neighbors_labels, 0,\n","                                                 one_hot_matrix_to_plot)\n","    plt.figure(figsize=(12, 10))\n","    if 5 not in np.unique(one_hot_matrix_to_plot):\n","        one_hot_matrix_to_plot = np.vstack((one_hot_matrix_to_plot, np.full((1, one_hot_matrix_to_plot.shape[1]), 5)))\n","        im = plt.imshow(one_hot_matrix_to_plot[:-1,:], cmap='bwr', aspect='auto')\n","        values = ['None', 'Predicted_labels_next_movie', 'true_label', 'predicted_label_is_true_label']\n","    else:\n","        im = plt.imshow(one_hot_matrix_to_plot, cmap='bwr', aspect='auto')\n","        values = ['None', 'Predicted_labels_next_movie', 'true_label', 'predicted_label_is_true_label', 'Predicted_label_errors']\n","    skip_x_ticks = math.ceil(len(continues_labels) / 15)\n","    x_array = np.arange(0, len(continues_labels), skip_x_ticks)\n","    print(\"labelneworder\")\n","    print(labels_new_order)\n","    y_labels = [label_decoder_dict[label_code] for label_code in labels_new_order]\n","    plt.ylim(len(y_labels), -0.3)\n","    plt.xticks(x_array, x_array, fontsize=10)\n","    plt.yticks(np.arange(len(labels_new_order)), y_labels, fontsize=10)\n","    # ==== create coustomize legand to the heat map =====\n","    colors = [im.cmap(im.norm(value)) for value in range(len(values))]\n","    patches = [mpatches.Patch(color=colors[i], label=values[i], edgecolor='b') for i in\n","               range(len(values))]\n","    plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.5, frameon=True)\n","    plt.title('Label Prediction in each frame', fontsize=14)\n","    plt.savefig(os.path.join(save_path_plots, 'change_in_accuracy_with_the_movment_of_sliding_window.png'), dpi=300,\n","                bbox_inches='tight')\n","    plt.close()\n","\n","\n","def create_one_hot_vector_matrix(array, array_max):\n","    one_hot_array = np.zeros((array.size, array_max + 1))\n","    one_hot_array[np.arange(array.size), array] = 1\n","    one_hot_array = one_hot_array.transpose()\n","    return one_hot_array\n","\n","\n","def resort_matrix(labels_order, matrix):\n","    # print(labels_order)\n","    # print(matrix.shape)\n","    \n","    sorted_matrix = np.zeros(matrix.shape)\n","    classes_that_we_plotted = []\n","    for row_index, label in enumerate(labels_order):\n","        if label.item() in classes_that_we_plotted:\n","            pass\n","        else:\n","            sorted_matrix[row_index] = matrix[label.item()]\n","            classes_that_we_plotted += [label.item()]\n","    index_of_filled_rows = row_index + 1\n","\n","    for index in range(len(matrix)):\n","        if index in classes_that_we_plotted:\n","            pass\n","        else:\n","            sorted_matrix[index_of_filled_rows] = matrix[index]\n","            index_of_filled_rows += 1\n","            if np.nonzero(matrix[index])[0].size != 0 and index not in classes_that_we_plotted:\n","                classes_that_we_plotted += [index]\n","    return sorted_matrix, classes_that_we_plotted\n","\n","\n","def increase_the_error_value_for_non_neighbors_labels(matrix_col):\n","    indices_of_non_zero_elements = np.nonzero(matrix_col)\n","    if len(indices_of_non_zero_elements[0]) > 1:\n","        dist_between_indices = indices_of_non_zero_elements[0][1] - indices_of_non_zero_elements[0][0]\n","        if dist_between_indices > 1:\n","            matrix_col[matrix_col == 1] = 5\n","    return matrix_col\n","\n","\n","def print_dataset_type_error():\n","    print(\n","        'You have enter a wrong dataset type in the dataset function. please fix it. possabilites are youtube or UCF101(the default)')\n","    sys.exit()\n","\n","\n","def plot_sliding_window_prediction_for_each_frame_no_labels(predicted_labels, save_path_plots, label_decoder_dict):\n","    original_order_of_labels = []\n","    for label in predicted_labels:\n","        if label.item() in original_order_of_labels:\n","            pass\n","        else:\n","            original_order_of_labels += [label]\n","    one_hot_matrix_to_plot = create_one_hot_vector_matrix(predicted_labels.numpy(), max(predicted_labels).item())\n","    one_hot_matrix_to_plot, ____ = resort_matrix(original_order_of_labels, one_hot_matrix_to_plot)\n","    one_hot_matrix_to_plot = one_hot_matrix_to_plot[~np.all(one_hot_matrix_to_plot == 0, axis=1)]\n","    fig, ax = plt.subplots(figsize=(12, 10))\n","    im = ax.imshow(one_hot_matrix_to_plot, cmap='GnBu', aspect='auto')\n","    skip_x_ticks = math.ceil(len(predicted_labels) / 15)\n","    x_array = np.arange(0, len(predicted_labels), skip_x_ticks)\n","    y_labels = [label_decoder_dict[label_code.item()] for label_code in original_order_of_labels]\n","    y_ticks = np.arange(len(original_order_of_labels))\n","    ax.set_ylim(len(y_labels), -0.3)\n","    ax.set_xticklabels(x_array, fontsize=10)\n","    ax.set_yticks(y_ticks)\n","    ax.set_yticklabels(y_labels, fontsize=10)\n","    # ==== create coustomize legand to the heat map =====\n","    values = ['None', 'Predicted_labels']\n","    colors = [im.cmap(im.norm(value)) for value in range(len(values))]\n","    patches = [mpatches.Patch(color=colors[i], label=values[i], edgecolor='b') for i in\n","               range(len(values))]\n","    plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.5, frameon=True)\n","    plt.title('Label Prediction in each frame', fontsize=14)\n","    plt.savefig(os.path.join(save_path_plots, 'change_in_accuracy_with_the_movement_of_sliding_window.png'), dpi=300,\n","                bbox_inches='tight')\n","\n","\n","def print_error_preprocessing_movie_mode():\n","    print('Your value in the pre-processing movie mode is incorrect. your options are:\\n'\n","          '1. live pre-processing.\\n'\n","          '2. pre-processied movie. \\n'\n","          'please choose one of them')\n","    sys.exit()\n","\n","\n","def predict_labels_of_sliding_window(sliding_window_images, batch_size, device, model):\n","    predicted_labels_list = []\n","    for batch_boundaries in range(0, len(sliding_window_images), batch_size):\n","        batch_images_to_plot = sliding_window_images[batch_boundaries: batch_boundaries + batch_size].to(device)\n","        predicted_labels = foward_step_no_labels(model, batch_images_to_plot)\n","        predicted_labels_list += [predicted_labels]\n","    return torch.cat(predicted_labels_list, axis=0)\n","\n","\n","def set_text_to_video_frame(continues_labels, label_decoder_dict, predicted_labels, mode, h_text_object=None,\n","                            frame='start', bool_array=None):\n","    if frame == 'start':\n","        height = 0.07 if mode != 'youtube' else 0.12\n","        fontsize = 5 if mode!= 'youtube' else 8\n","        h_text_1 = plt.text(0.18, height, 'Predicted labels - {}'.format(label_decoder_dict[predicted_labels[0].item()]),\n","                            color='blue', fontsize=fontsize, transform=plt.gcf().transFigure)\n","        if continues_labels is not None:\n","            h_text_2 = plt.text(0.18, 0.11, 'Original_labels', color='black', fontsize=5,\n","                                transform=plt.gcf().transFigure)\n","            h_text_3 = plt.text(0.44, 0.01, 'True/False', color='red', fontsize=6, transform=plt.gcf().transFigure,\n","                                path_effects=[pe.withStroke(linewidth=1, foreground=\"black\")])\n","            return {index + 1: text_object for index, text_object in enumerate([h_text_1, h_text_2, h_text_3])}\n","        else:\n","            return {1: h_text_1}\n","    else:\n","        h_text_object[1].set_text('Predicted labels - {}'.format(label_decoder_dict[predicted_labels[frame].item()]))\n","        if continues_labels is not None:\n","            h_text_object[2].set_text('Original label - {}'.format(label_decoder_dict[continues_labels[frame].item()]))\n","            color = 'green' if bool_array[frame].item() else 'red'\n","            h_text_object[3].remove()\n","            h_text_object[3] = plt.text(0.44, 0.01, str(bool_array[frame].item()), color=color, fontsize=6,\n","                                        transform=plt.gcf().transFigure,\n","                                        path_effects=[pe.withStroke(linewidth=1, foreground=\"black\")])\n","\n","\n","def generate_list_of_colors(num_labels):\n","    green_color_codes = [[154, 205, 50], [85, 107, 47], [107, 142, 35], [124, 252, 0], [127, 255, 0], [173, 255, 47],\n","                         [0, 100, 0],\n","                         [0, 128, 0], [34, 139, 34], [0, 255, 0], [50, 205, 50], [144, 238, 144], [152, 251, 152],\n","                         [60, 179, 113],\n","                         [46, 139, 87], [0, 255, 127], [0, 250, 154]]\n","    color_list = []\n","    for i in range(num_labels):\n","        color = list(np.random.choice(range(256), size=3))\n","\n","        while color in color_list or color in green_color_codes:\n","            color = list(np.random.choice(range(256), size=3))\n","        color_norm = [single_color / 255 for single_color in color]\n","        color_list += [color_norm]\n","    color_list_tuple = [tuple(color_as_list) for color_as_list in color_list]\n","    return color_list_tuple\n","\n","\n","def create_color_dict(predicted_labels):\n","    unique_labels = predicted_labels.unique()\n","    color_list = generate_list_of_colors(len(unique_labels))\n","    color_dict = {}\n","    for index, label in enumerate(unique_labels):\n","        index_of_specific_label = (predicted_labels == label.item()).nonzero()\n","        if len(index_of_specific_label) > (0.5 * len(predicted_labels)):\n","            color_list[index] = 'green'\n","        color_dict[label.item()] = color_list[index]\n","    return color_dict\n","\n","\n","def save_video_original_size_dict(video_original_size_dict, save_path):\n","    with open(os.path.join(save_path, 'video_original_size_dict.pkl'), 'wb') as f:\n","        pickle.dump(video_original_size_dict, f, pickle.HIGHEST_PROTOCOL)\n","\n","def load_and_extract_video_original_size(filename):\n","    video = cv2.VideoCapture(filename)\n","    fps = video.get(cv2.CAP_PROP_FPS)\n","    video_width = video.get(cv2.CAP_PROP_FRAME_WIDTH)\n","    video_height = video.get(cv2.CAP_PROP_FRAME_HEIGHT)\n","    fname = filename.split(\".\")[0]\n","    dict = { fname : [video_width, video_height] }\n","    return dict\n","\n","\n","def setting_video_size(video_original_size):\n","    if video_original_size is None:\n","         (w, h) = (320, 240)\n","    else:\n","        (w, h) = (int(video_original_size[0]), int(video_original_size[1]))\n","    for size_element in [w, h]:\n","        if size_element % 2 == 0:\n","            size_element += 1\n","    return w, h"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"LREkmqYIfP9i","executionInfo":{"status":"ok","timestamp":1610585112525,"user_tz":360,"elapsed":9282,"user":{"displayName":"Roline Saldanha","photoUrl":"","userId":"08007127351234095824"}}},"source":["def main_procesing_data(args, folder_dir, sampled_video_file=None, processing_mode='main'):\r\n","    \"\"\"\"\r\n","       Create the sampled data video,\r\n","       input - video, full length.\r\n","       function - 1. Read the video using CV2\r\n","                  2. from each video X (args.sampling_rate) frames are sampled reducing the FPS by args.sampling_rate (for example from 25 to 2.5 FPS)\r\n","                  3. The function randomly set the start point where the new sampled video would be read from, and Y(args.num_frames_to_extract) continues frames are extracted.\r\n","                  4. if processing_mode == 'main' The Y continues frames are extracted and save to a new video if not the data in tensor tyoe mode is passed to the next function\r\n","       Output: videos in length of X frames\r\n","       \"\"\"\r\n","    if args.dataset == 'UCF101':\r\n","        for file_name in os.listdir(args.ucf_list_dir):\r\n","            # ===== reading all of the row data from the first split of train and test =====\r\n","            if '1' in file_name:\r\n","                with open(os.path.join(args.ucf_list_dir, file_name)) as f:\r\n","                    video_list = f.readlines()\r\n","                with tqdm(total=len(video_list)) as pbar:\r\n","                # with tqdm_notebook(total=len(dataloader)) as pbar:\r\n","                    for video_name in video_list:\r\n","                        video_name = video_name.split(' ')[0].rstrip('\\n')\r\n","                        capture_and_sample_video(args.row_data_dir, video_name, args.num_frames_to_extract,\r\n","                                                 args.sampling_rate, args.ucf101_fps, folder_dir,\r\n","                                                 args.ucf101_fps, processing_mode)\r\n","                        pbar.update(1)\r\n","            else:\r\n","                pass\r\n","\r\n","    elif args.dataset == 'youtube':\r\n","        video_original_size_dict = {}\r\n","\r\n","        if args.video_file_name is None and sampled_video_file is None:\r\n","            for file_name in os.listdir(args.row_data_dir):\r\n","                video_test, video_original_size = capture_and_sample_video(args.row_data_dir, file_name, 'all', args.sampling_rate, 'Not known',\r\n","                                         folder_dir, args.ucf101_fps, processing_mode)\r\n","                video_original_size_dict[file_name.split('.')[0]] = video_original_size\r\n","        else:\r\n","            file_name = args.video_file_name if sampled_video_file is None else sampled_video_file[0]\r\n","            print(\"Filename\", file_name)\r\n","            video_test, video_original_size = capture_and_sample_video(args.row_data_dir, file_name, 'all', args.sampling_rate, 'Not known',\r\n","                                     folder_dir, args.ucf101_fps, processing_mode)\r\n","            \r\n","            video_original_size_dict[file_name.split('.')[0]] = video_original_size\r\n","        save_video_original_size_dict(video_original_size_dict, folder_dir)\r\n","        if processing_mode == 'live':\r\n","            return video_test, video_original_size\r\n","            \r\n","class UCF101Dataset(Dataset):\r\n","    def __init__(self, data_path, data, mode, dataset='UCF101'):\r\n","        super(UCF101Dataset, self).__init__()\r\n","        self.dataset = dataset\r\n","        if self.dataset == 'UCF101':\r\n","            self.labels = data[1]\r\n","        self.data_path = data_path\r\n","        self.images = data[0]\r\n","        self.transform = set_transforms(mode)\r\n","\r\n","    # ====== Override to give PyTorch size of dataset ======\r\n","    def __len__(self):\r\n","        return len(self.images)\r\n","\r\n","    def __getitem__(self, idx):\r\n","        if self.dataset == 'UCF101':\r\n","            #sampled_video_name = self.images[idx].split('/')[1] +'.avi'\r\n","            sampled_video_name = self.images[idx] +'.avi'\r\n","            print(sampled_video_name)\r\n","        elif self.dataset == 'youtube':\r\n","            sampled_video_name = self.images[idx]\r\n","        else:\r\n","           print_dataset_type_error()\r\n","        # ====== extract numpy array from the video and sample it so we will have an array with lower FPS rate =======\r\n","        video_frames = skvideo.io.vread(os.path.join(self.data_path, sampled_video_name))\r\n","        video_frames_array = []\r\n","        for image in video_frames:\r\n","            img = Image.fromarray(image.astype('uint8'), 'RGB')\r\n","            img = self.transform(img)\r\n","            video_frames_array.append(img)\r\n","        img_stack = torch.stack(video_frames_array)\r\n","        #print(img_stack.shape)\r\n","        if self.dataset == 'UCF101':\r\n","            label = torch.from_numpy(np.asarray(int(self.labels[idx]))).long()\r\n","            return img_stack, label, idx\r\n","        else:\r\n","            return img_stack\r\n","\r\n","\r\n","\r\n","class UCF101DatasetSampler(Sampler):\r\n","    def __init__(self, data, batch_size):\r\n","        self.num_samples = len(data)\r\n","        self.classes_that_were_sampled = []\r\n","        self.data_labels = data.labels\r\n","        self.batch_size = batch_size\r\n","\r\n","\r\n","    def __iter__(self):\r\n","        idx_list = []\r\n","        for i in range(self.batch_size):\r\n","            idx_image_sample = sample(range(self.num_samples), 1)[0]\r\n","            label_sample = self.data_labels[idx_image_sample]\r\n","            while label_sample in self.classes_that_were_sampled:\r\n","                idx_image_sample = sample(range(self.num_samples), 1)[0]\r\n","                label_sample = self.data_labels[idx_image_sample]\r\n","            self.classes_that_were_sampled += [label_sample]\r\n","            idx_list += [idx_image_sample]\r\n","        return iter(idx_list)\r\n","\r\n","    def __len__(self):\r\n","        return self.num_samples\r\n","\r\n","class ConvLstm(nn.Module):\r\n","    def __init__(self, latent_dim, hidden_size, lstm_layers, bidirectional, n_class):\r\n","        super(ConvLstm, self).__init__()\r\n","        self.conv_model = Pretrained_conv(latent_dim)\r\n","        self.Lstm = Lstm(latent_dim, hidden_size, lstm_layers, bidirectional)\r\n","        self.output_layer = nn.Sequential(\r\n","            nn.Linear(2 * hidden_size if bidirectional==True else hidden_size, n_class),\r\n","            nn.Softmax()\r\n","            #nn.Softmax(dim=-1)\r\n","        )\r\n","\r\n","    def forward(self, x):\r\n","        batch_size, timesteps, channel_x, h_x, w_x = x.shape\r\n","        conv_input = x.view(batch_size * timesteps, channel_x, h_x, w_x)\r\n","        conv_output = self.conv_model(conv_input)\r\n","        lstm_input = conv_output.view(batch_size, timesteps, -1)\r\n","        lstm_output = self.Lstm(lstm_input)\r\n","        lstm_output = lstm_output[:, -1, :]\r\n","        output = self.output_layer(lstm_output)\r\n","        return output\r\n","\r\n","class Pretrained_conv(nn.Module):\r\n","    def __init__(self, latent_dim):\r\n","        super(Pretrained_conv, self).__init__()\r\n","        self.conv_model = models.resnet152(pretrained=True)\r\n","        # ====== freezing all of the layers ======\r\n","        for param in self.conv_model.parameters():\r\n","            param.requires_grad = False\r\n","        # ====== changing the last FC layer to an output with the size we need. this layer is un freezed ======\r\n","        self.conv_model.fc = nn.Linear(self.conv_model.fc.in_features, latent_dim)\r\n","\r\n","    def forward(self, x):\r\n","        return self.conv_model(x)\r\n","\r\n","class Lstm(nn.Module):\r\n","    def __init__(self, latent_dim, hidden_size, lstm_layers, bidirectional):\r\n","        super(Lstm, self).__init__()\r\n","        self.Lstm = nn.LSTM(latent_dim, hidden_size=hidden_size, num_layers=lstm_layers, batch_first=True, bidirectional=bidirectional)\r\n","        self.hidden_state = None\r\n","\r\n","    def reset_hidden_state(self):\r\n","        self.hidden_state = None\r\n","\r\n","    def forward(self,x):\r\n","        output, self.hidden_state = self.Lstm(x, self.hidden_state)\r\n","        return output\r\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2EXc0sc1iOH6","executionInfo":{"status":"ok","timestamp":1610585113774,"user_tz":360,"elapsed":351,"user":{"displayName":"Roline Saldanha","photoUrl":"","userId":"08007127351234095824"}},"outputId":"a4290692-f28c-4d93-8626-9b536a5b4376"},"source":["parser = argparse.ArgumentParser(description='UCF101 Action Recognition, LRCN architecture')\r\n","\r\n","#place your model checkpoint path here\r\n","parser.add_argument('--model_dir', default=r'/content/drive/My Drive/directed studies/20201002-235444/Saved_model_checkpoints/', type=str, help='The dir of the model we want to test')\r\n","parser.add_argument('--model_name', default='epoch_15.pth.tar', type=str, help='the name for the model we want to test on')\r\n","\r\n","#put your test video path here\r\n","parser.add_argument('--video_file_name', default=\"test_video.mp4\", type=str,\r\n","                    help='the video file name we would process, if none the script would run on all of the video files in the folder')\r\n","\r\n","#keep it 'live' so that the code preprocesses it\r\n","parser.add_argument('--preprocessing_movie_mode', default='live', type=str,\r\n","                    help='should we preprocess the video on the go (live) or using the preprocessed script (default:live, options: live/preprocessed)')\r\n","parser.add_argument('--dataset', default='youtube', type=str,\r\n","                    help='the dataset name. options = youtube, UCF101')\r\n","parser.add_argument('--sampling_rate', default=1, type=int, help='what was the sampling rate of the ucf-101 train dataset')\r\n","parser.add_argument('--ucf101_fps', default=25, type=int, help='FPS of the UCF101 dataset')\r\n","\r\n","parser.add_argument('--row_data_dir', default=r'/content/drive/My Drive/directed studies/test/', type=str,\r\n","                    help='path to find the UCF101 row data')\r\n","\r\n","parser.add_argument('--open_new_folder', default='True', type=str,\r\n","                    help='open a new folder for saving the run info, if false the info would be saved in the project '\r\n","                         'dir, if debug the info would be saved in debug folder(default:True)')\r\n","parser.add_argument('--number_of_classes', default=2, type=int, help='The number of classes we would train on')\r\n","parser.add_argument('--latent_dim', default=512, type=int, help='The dim of the Conv FC output (default:512)')\r\n","parser.add_argument('--hidden_size', default=256, type=int,\r\n","                    help=\"The number of features in the LSTM hidden state (default:256)\")\r\n","parser.add_argument('--lstm_layers', default=2, type=int, help='Number of recurrent layers (default:2)')\r\n","parser.add_argument('--bidirectional', default=True, type=bool, help='set the LSTM to be bidirectional (default:True)')\r\n","parser.add_argument('--sampled_data_dir',\r\n","                    default=r'/content/drive/My Drive/directed studies/test/',\r\n","                    type=str, help='The dir for the sampled row data')\r\n","parser.add_argument('--batch-size', default=1, type=int, help='mini-batch size (default:32)')\r\n","parser.add_argument('--load_all_data_to_RAM', default=False, type=bool,\r\n","                    help='load dataset directly to the RAM, for faster computation. usually use when the num of class '\r\n","                         'is small (default:False')"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["_StoreAction(option_strings=['--load_all_data_to_RAM'], dest='load_all_data_to_RAM', nargs=None, const=None, default=False, type=<class 'bool'>, choices=None, help='load dataset directly to the RAM, for faster computation. usually use when the num of class is small (default:False', metavar=None)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"vJW5JBPNzPTR","executionInfo":{"status":"ok","timestamp":1610585115040,"user_tz":360,"elapsed":581,"user":{"displayName":"Roline Saldanha","photoUrl":"","userId":"08007127351234095824"}}},"source":["def main():\n","    # ====== set the run settings ======\n","    args = parser.parse_args(\"\")\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","    folder_dir = set_project_folder_dir(args.open_new_folder, args.model_dir, use_model_folder_dir=True, mode='test_youtube_movie')\n","\n","    print('The setting of the run are:\\n{}\\n' .format(args))\n","    print('The training would take place on {}\\n'.format(device))\n","    print('The project directory is {}' .format(folder_dir))\n","\n","    save_setting_info(args, device, folder_dir)\n","    label_decoder_dict = load_test_data(args.model_dir, mode='load_label_decoder_dict')\n","\n","    print('Loading model...')\n","    num_class = len(label_decoder_dict) if args.number_of_classes is None else args.number_of_classes\n","    model = ConvLstm(args.latent_dim, args.hidden_size, args.lstm_layers, args.bidirectional, num_class)\n","    model = model.to(device)\n","    checkpoint = torch.load(os.path.join(args.model_dir, args.model_name))\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","    # ====== inferance_mode ======\n","    if args.video_file_name is None and args.preprocessing_movie_mode != 'live':\n","        test_videos_names = [file_name for file_name in os.listdir(args.sampled_data_dir) if '.avi' in file_name]\n","\n","    elif args.video_file_name is None:\n","        test_videos_names = [file_name for file_name in os.listdir(args.row_data_dir)]\n","\n","    else:\n","        test_videos_names = [args.video_file_name]\n","\n","    print(test_videos_names)\n","\n","    if args.preprocessing_movie_mode == 'preprocessed':\n","        dataset = UCF101Dataset(args.sampled_data_dir, [test_videos_names], mode='test', dataset='youtube')\n","        dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n","        video_original_size_dict = load_and_extract_video_original_size(args.sampled_data_dir)\n","        test_model_continues_movie_youtube(model, dataloader, device, folder_dir, label_decoder_dict, args.batch_size,\n","                                           args.preprocessing_movie_mode, video_original_size=video_original_size_dict)\n","    elif args.preprocessing_movie_mode == 'live':\n","        movie_name_to_test = sample(test_videos_names, 1)\n","        \n","        test_movie, video_original_size = main_procesing_data(args, folder_dir, sampled_video_file=None,\n","                                                              processing_mode='live')\n","        moments_in_seconds = test_model_continues_movie_youtube(model, torch.stack(test_movie), device, folder_dir, label_decoder_dict,\n","                                           args.batch_size, args.preprocessing_movie_mode,\n","                                           video_original_size=video_original_size, vid=args.video_file_name, pth=args.row_data_dir)\n","    else:\n","        print_error_preprocessing_movie_mode()\n","    return moments_in_seconds\n","\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"CwT-qBeYxof4","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["22c798b1c9844af9bf08351ab159eb3e","0b14120e48354d519caeb78fd93393d4","935c24cf95ea41c58bc6da4a3073f39c","e223850c5a7b43ab823b67cafa7b3ad2","f21fcc18dc2a45b8b8932231f4035930","84eafa7a7f1d47038167cdcf2baf5c3f","39c10a430331457a9f6fbbe59d64948f","cc53055bebf14a5ebfc9e8daf5fbe49b"]},"executionInfo":{"status":"ok","timestamp":1610585188773,"user_tz":360,"elapsed":65667,"user":{"displayName":"Roline Saldanha","photoUrl":"","userId":"08007127351234095824"}},"outputId":"18bada01-34a3-4246-f9e8-abf3654e348e"},"source":["moments_in_seconds = main()\n","print(moments_in_seconds)\n","\n","#Save the moments in a file\n","with open('/content/drive/My Drive/directed studies/testmoments.pkl', 'wb') as fp:\n","    pickle.dump(moments_in_seconds, fp)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["The setting of the run are:\n","Namespace(batch_size=1, bidirectional=True, dataset='youtube', hidden_size=256, latent_dim=512, load_all_data_to_RAM=False, lstm_layers=2, model_dir='/content/drive/My Drive/directed studies/20201002-235444/Saved_model_checkpoints/', model_name='epoch_15.pth.tar', number_of_classes=2, open_new_folder='True', preprocessing_movie_mode='live', row_data_dir='/content/drive/My Drive/directed studies/test/', sampled_data_dir='/content/drive/My Drive/directed studies/test/', sampling_rate=1, ucf101_fps=25, video_file_name='test_video.mp4')\n","\n","The training would take place on cuda:0\n","\n","The project directory is /content/drive/My Drive/directed studies/20201002-235444/test_youtube_movie\n","Videopath []\n","Loading model...\n"],"name":"stdout"},{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-b121ed2d.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"22c798b1c9844af9bf08351ab159eb3e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=241530880.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","['test_video.mp4']\n","Filename test_video.mp4\n","FPS : 29.97223160895053\n","total number of frames 544\n","Details:  0 1 544\n","frames to extract 544\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"],"name":"stderr"},{"output_type":"stream","text":["Len of predicted labels 540\n","f_labelslen 545\n","[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","[[96, 181], [296, 318], [322, 325], [335, 355], [365, 450], [477, 533]]\n","[[3.202964705882353, 6.038923039215686], [9.875807843137256, 10.609820588235294], [10.743277450980393, 10.843370098039216], [11.17701225490196, 11.84429656862745], [12.177938725490197, 15.01389705882353], [15.914730882352941, 17.783126960784315]]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:865: UserWarning: Setting the 'color' property will override the edgecolor or facecolor properties.\n"],"name":"stderr"},{"output_type":"stream","text":["[[3.202964705882353, 6.038923039215686], [9.875807843137256, 10.609820588235294], [10.743277450980393, 10.843370098039216], [11.17701225490196, 11.84429656862745], [12.177938725490197, 15.01389705882353], [15.914730882352941, 17.783126960784315]]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x720 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"ei7n5SIAbOEx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610571645829,"user_tz":360,"elapsed":437,"user":{"displayName":"Roline Saldanha","photoUrl":"","userId":"08007127351234095824"}},"outputId":"d0cfd574-46e8-4a9e-feca-49897c67877b"},"source":["#load the moments from the file\n","import pickle\n","with open ('/content/drive/My Drive/directed studies/testmoments.pkl', 'rb') as fp:\n","    itemlist = pickle.load(fp)\n","\n","print(itemlist)\n","print(len(itemlist))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["[[3.202964705882353, 6.038923039215686], [9.875807843137256, 10.609820588235294], [10.743277450980393, 10.843370098039216], [11.17701225490196, 11.84429656862745], [12.177938725490197, 15.01389705882353], [15.914730882352941, 17.783126960784315]]\n","6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"084bMrDi8x2C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610247693545,"user_tz":360,"elapsed":451,"user":{"displayName":"Roline Saldanha","photoUrl":"","userId":"08007127351234095824"}},"outputId":"7dc39bd9-efa5-43e3-f47b-27ece6562172"},"source":["#merge the moments that are less than 1 second apart\n","result = []\n","result.append(itemlist[0])\n","for item in itemlist[1:]:\n","  if item[0] - result[-1][1] < 1:\n","    result[-1][1] = item[1]\n","  else:\n","    result.append(item)\n","\n","print(result)\n","print(len(result))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[1.2095416666666665, 1.4597916666666666], [2.6693333333333333, 3.294958333333333], [8.75875, 8.75875], [10.218541666666665, 14.347666666666665], [18.5185, 54.72133333333333], [59.184124999999995, 60.64391666666666], [62.52079166666666, 64.5645], [66.191125, 67.442375], [68.86045833333333, 71.61320833333333], [76.74333333333333, 82.54079166666666], [85.460375, 98.05629166666667], [99.64120833333332, 101.26783333333333], [102.477375, 102.477375], [108.39995833333333, 129.33754166666665], [141.266125, 141.30783333333332], [142.517375, 203.53666666666666], [208.208, 208.37483333333333], [210.54366666666664, 210.91904166666666], [216.841625, 217.71749999999997], [237.82091666666665, 238.90533333333332], [248.53995833333332, 248.53995833333332], [251.7515, 253.83691666666664]]\n","22\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_5jI3-yGbyCw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610247705064,"user_tz":360,"elapsed":625,"user":{"displayName":"Roline Saldanha","photoUrl":"","userId":"08007127351234095824"}},"outputId":"29bc070e-0df4-4af9-aef1-90672d316d7e"},"source":["#Remove the moments that are shorter than 1 second and create the json format that is needed for ilab website\n","#change the videoId accordingly\n","\n","\n","moments_json = {\n","    \"videoId\" : \"1uNq_9wb_mo\",\n","    \"type\" : \"segment\",\n","    \"startTime\" : 0,\n","    \"endTime\" : 0,\n","    \"observer\" : \"LCRN model\",\n","    \"isHuman\" : False,\n","    \"confirmedBySomeone\" : False,\n","    \"rejectedBySomeone\" : False,\n","    \"observation\" : {\n","        \"label\" : \"Smoking\",\n","        \"labelConfidence\" : 0.89\n","    }\n","\n","}\n","\n","moment_list = []\n","for moment in result:\n","  if moment[1] - moment[0] < 1:\n","    continue\n","  moments_json[\"startTime\"] = round(moment[0], 3)\n","  moments_json[\"endTime\" ] = round(moment[1], 3)\n","  print(moment)\n","  moment_list.append(moments_json.copy())\n","\n","print(moment_list)\n","print(len(moment_list))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[10.218541666666665, 14.347666666666665]\n","[18.5185, 54.72133333333333]\n","[59.184124999999995, 60.64391666666666]\n","[62.52079166666666, 64.5645]\n","[66.191125, 67.442375]\n","[68.86045833333333, 71.61320833333333]\n","[76.74333333333333, 82.54079166666666]\n","[85.460375, 98.05629166666667]\n","[99.64120833333332, 101.26783333333333]\n","[108.39995833333333, 129.33754166666665]\n","[142.517375, 203.53666666666666]\n","[237.82091666666665, 238.90533333333332]\n","[251.7515, 253.83691666666664]\n","[{'videoId': '1uNq_9wb_mo', 'type': 'segment', 'startTime': 10.219, 'endTime': 14.348, 'observer': 'LCRN model', 'isHuman': False, 'confirmedBySomeone': False, 'rejectedBySomeone': False, 'observation': {'label': 'Smoking', 'labelConfidence': 0.89}}, {'videoId': '1uNq_9wb_mo', 'type': 'segment', 'startTime': 18.518, 'endTime': 54.721, 'observer': 'LCRN model', 'isHuman': False, 'confirmedBySomeone': False, 'rejectedBySomeone': False, 'observation': {'label': 'Smoking', 'labelConfidence': 0.89}}, {'videoId': '1uNq_9wb_mo', 'type': 'segment', 'startTime': 59.184, 'endTime': 60.644, 'observer': 'LCRN model', 'isHuman': False, 'confirmedBySomeone': False, 'rejectedBySomeone': False, 'observation': {'label': 'Smoking', 'labelConfidence': 0.89}}, {'videoId': '1uNq_9wb_mo', 'type': 'segment', 'startTime': 62.521, 'endTime': 64.564, 'observer': 'LCRN model', 'isHuman': False, 'confirmedBySomeone': False, 'rejectedBySomeone': False, 'observation': {'label': 'Smoking', 'labelConfidence': 0.89}}, {'videoId': '1uNq_9wb_mo', 'type': 'segment', 'startTime': 66.191, 'endTime': 67.442, 'observer': 'LCRN model', 'isHuman': False, 'confirmedBySomeone': False, 'rejectedBySomeone': False, 'observation': {'label': 'Smoking', 'labelConfidence': 0.89}}, {'videoId': '1uNq_9wb_mo', 'type': 'segment', 'startTime': 68.86, 'endTime': 71.613, 'observer': 'LCRN model', 'isHuman': False, 'confirmedBySomeone': False, 'rejectedBySomeone': False, 'observation': {'label': 'Smoking', 'labelConfidence': 0.89}}, {'videoId': '1uNq_9wb_mo', 'type': 'segment', 'startTime': 76.743, 'endTime': 82.541, 'observer': 'LCRN model', 'isHuman': False, 'confirmedBySomeone': False, 'rejectedBySomeone': False, 'observation': {'label': 'Smoking', 'labelConfidence': 0.89}}, {'videoId': '1uNq_9wb_mo', 'type': 'segment', 'startTime': 85.46, 'endTime': 98.056, 'observer': 'LCRN model', 'isHuman': False, 'confirmedBySomeone': False, 'rejectedBySomeone': False, 'observation': {'label': 'Smoking', 'labelConfidence': 0.89}}, {'videoId': '1uNq_9wb_mo', 'type': 'segment', 'startTime': 99.641, 'endTime': 101.268, 'observer': 'LCRN model', 'isHuman': False, 'confirmedBySomeone': False, 'rejectedBySomeone': False, 'observation': {'label': 'Smoking', 'labelConfidence': 0.89}}, {'videoId': '1uNq_9wb_mo', 'type': 'segment', 'startTime': 108.4, 'endTime': 129.338, 'observer': 'LCRN model', 'isHuman': False, 'confirmedBySomeone': False, 'rejectedBySomeone': False, 'observation': {'label': 'Smoking', 'labelConfidence': 0.89}}, {'videoId': '1uNq_9wb_mo', 'type': 'segment', 'startTime': 142.517, 'endTime': 203.537, 'observer': 'LCRN model', 'isHuman': False, 'confirmedBySomeone': False, 'rejectedBySomeone': False, 'observation': {'label': 'Smoking', 'labelConfidence': 0.89}}, {'videoId': '1uNq_9wb_mo', 'type': 'segment', 'startTime': 237.821, 'endTime': 238.905, 'observer': 'LCRN model', 'isHuman': False, 'confirmedBySomeone': False, 'rejectedBySomeone': False, 'observation': {'label': 'Smoking', 'labelConfidence': 0.89}}, {'videoId': '1uNq_9wb_mo', 'type': 'segment', 'startTime': 251.751, 'endTime': 253.837, 'observer': 'LCRN model', 'isHuman': False, 'confirmedBySomeone': False, 'rejectedBySomeone': False, 'observation': {'label': 'Smoking', 'labelConfidence': 0.89}}]\n","13\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VFcWzbIVWWF9"},"source":["import json\r\n","with open('/content/drive/My Drive/directed studies/testvideos/filename.json', 'w') as fout:\r\n","    json.dump(moment_list, fout)"],"execution_count":null,"outputs":[]}]}