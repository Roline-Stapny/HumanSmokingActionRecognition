{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pre_process.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMxVgQ7kAGV5OuodU6nD/Dd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"njzZ9fjyrCgW"},"source":["This Notebook is used to preprocess the training data. It takes video as the input and returns a sampled video"]},{"cell_type":"code","metadata":{"id":"aaT9WK1_TuYu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610564276997,"user_tz":360,"elapsed":26748,"user":{"displayName":"Roline Saldanha","photoUrl":"","userId":"08007127351234095824"}},"outputId":"7f601f81-3908-4a92-b148-48996487c053"},"source":["from google.colab import drive\n","drive.flush_and_unmount\n","drive.mount(\"/content/drive/\", force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ia16gnBxpT8F","executionInfo":{"status":"ok","timestamp":1610564283671,"user_tz":360,"elapsed":3894,"user":{"displayName":"Roline Saldanha","photoUrl":"","userId":"08007127351234095824"}},"outputId":"00b0ba74-2f12-444d-ed2c-50907c19753f"},"source":["!pip install scikit-video"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting scikit-video\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/a6/c69cad508139a342810ae46e946ebb3256aa6e42f690d901bb68f50582e3/scikit_video-1.1.11-py2.py3-none-any.whl (2.3MB)\n","\u001b[K     |████████████████████████████████| 2.3MB 16.1MB/s \n","\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from scikit-video) (7.0.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from scikit-video) (1.4.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-video) (1.19.5)\n","Installing collected packages: scikit-video\n","Successfully installed scikit-video-1.1.11\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xtEPU03PaaWG","executionInfo":{"status":"ok","timestamp":1610564288338,"user_tz":360,"elapsed":5120,"user":{"displayName":"Roline Saldanha","photoUrl":"","userId":"08007127351234095824"}}},"source":["import time\n","import os\n","\n","import cv2\n","import random\n","\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","\n","import math\n","from tqdm import tqdm\n","\n","from random import sample\n","import numpy as np\n","from collections import Counter\n","import sys\n","from PIL import Image\n","\n","\n","def set_project_folder_dir(if_open_new_folder, local_dir, use_model_folder_dir=False, mode=None):\n","    if use_model_folder_dir:\n","        folder_dir = os.path.join(os.path.normpath(local_dir + os.sep + os.pardir), mode)\n","        create_folder_dir_if_needed(folder_dir)\n","    else:\n","        if if_open_new_folder != 'False':\n","            folder_dir = open_new_folder(if_open_new_folder, local_dir)\n","        else:\n","            folder_dir = local_dir\n","    return folder_dir\n","\n","\n","def open_new_folder(if_open_new_folder, local_dir):\n","    if if_open_new_folder == 'True':\n","        folder_name = time.strftime(\"%Y%m%d-%H%M%S\")\n","    else:\n","        folder_name = 'debug'\n","    folder_dir = os.path.join(local_dir, folder_name)\n","    create_folder_dir_if_needed(folder_dir)\n","    return folder_dir\n","\n","\n","def save_setting_info(args, device, folder_dir):\n","    setting_file_name = os.path.join(folder_dir, 'setting_info.txt')\n","    args_dict = args.__dict__\n","    with open(setting_file_name, 'w') as f:\n","        for key, value in args_dict.items():\n","            f.write(key + ' : ' + str(value) + '\\n')\n","        f.write(str(device))\n","\n","\n","\n","\n","def get_data(mode, video_names, list, number_of_classes, labels=[]):\n","    # setting the data files as a list so the not overpower the system\n","    for video_name in video_names:\n","        if mode == 'train':\n","            video_name, label = video_name.split(' ')\n","            label = int(label.rstrip('\\n'))\n","            if number_of_classes is None or label in range(1, number_of_classes + 1):\n","                labels.append(label - 1)\n","                list.append(video_name.split('.')[0])\n","            else:\n","                continue\n","        else:\n","            list.append(video_name.split('.')[0])\n","    return list, labels\n","\n","\n","def get_video_list(ucf_list_root, number_of_classes, folder_dir):\n","    # ====== get a list of video names ======\n","    video_names_train, video_names_test, labels = [], [], []\n","    sample_train_test_split = str(sample(range(1, 4), 1)[0])\n","    with open(os.path.join(folder_dir, 'setting_info.txt'), 'a+') as f:\n","        f.write('\\nThe test/train split that we have train on is {}'.format(sample_train_test_split))\n","    for file_name in os.listdir(ucf_list_root):\n","        file_path = os.path.join(ucf_list_root, file_name)\n","        if 'train' in file_name and sample_train_test_split in file_name:\n","            with open(file_path) as f:\n","                video_names = f.readlines()\n","            video_names_train, labels = get_data('train', video_names, video_names_train, number_of_classes, labels)\n","        elif 'classInd' in file_name:\n","            with open(file_path) as f:\n","                labels_decoder = f.readlines()\n","            labels_decoder_dict = {int(x.split(' ')[0]) - 1: x.split(' ')[1].rstrip('\\n') for x in labels_decoder}\n","        elif 'test' in file_name and sample_train_test_split in file_name:\n","            with open(file_path) as f:\n","                video_names = f.readlines()\n","            video_names_test, _ = get_data('test', video_names, video_names_test, number_of_classes)\n","\n","    return video_names_train, video_names_test, labels, labels_decoder_dict\n","\n","\n","\n","\n","def create_folder_dir_if_needed(folder_save_dir):\n","    if not os.path.exists(folder_save_dir):\n","        os.makedirs(folder_save_dir)\n","\n","\n","\n","\n","\n","def set_transforms(mode):\n","    if mode == 'train':\n","        transform = transforms.Compose(\n","            [transforms.Resize(256),  # this is set only because we are using Imagenet pre-train model.\n","             transforms.RandomCrop(224),\n","             transforms.RandomHorizontalFlip(),\n","             transforms.ToTensor(),\n","             transforms.Normalize(mean=(0.485, 0.456, 0.406),\n","                                  std=(0.229, 0.224, 0.225))\n","             ])\n","    elif mode == 'test' or mode == 'val':\n","        transform = transforms.Compose([transforms.Resize((224, 224)),\n","                                        transforms.ToTensor(),\n","                                        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n","                                                             std=(0.229, 0.224, 0.225))])\n","    return transform\n","\n","\n","def create_new_video(save_path, video_name, image_array):\n","    \n","    (h, w) = image_array[0].shape[:2]\n","    if len(video_name.split('/')) > 1:\n","        video_name = video_name.split('/')[1]\n","    else:\n","        video_name = video_name.split('.mp4')[0]\n","        video_name = video_name + '.avi'\n","    save_video_path = os.path.join(save_path, video_name)\n","    output_video = cv2.VideoWriter(save_video_path, cv2.VideoWriter_fourcc(*'MJPG'), 5, (w, h), True)\n","    for frame in range(len(image_array)):\n","        output_video.write(image_array[frame])\n","    output_video.release()\n","    cv2.destroyAllWindows()\n","\n","\n","#########\n","def setting_sample_rate(num_frames_to_extract, sampling_rate, video, fps, ucf101_fps):\n","    video.set(cv2.CAP_PROP_POS_AVI_RATIO, 1)\n","    \n","    # video_length = video.get(cv2.CAP_PROP_POS_MSEC)\n","    # print(\"VideoLenght\", video_length)\n","\n","    cal_fps = video.get(cv2.CAP_PROP_FPS)\n","\n","    print(\"FPS :\",cal_fps)\n","    \n","    \n","    num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n","    # print( num_frames )\n","\n","    # num_frames = int(video_length * fps)\n","    # print(\"number of frames\", num_frames)\n","\n","    if num_frames_to_extract == 'all':\n","        sample_start_point = 0\n","        if fps != ucf101_fps and sampling_rate != 0:\n","            sampling_rate = math.ceil(fps / (ucf101_fps / sampling_rate))\n","    elif num_frames < (num_frames_to_extract * sampling_rate):\n","        sample_start_point = 5\n","        sampling_rate = 3\n","    else:\n","        # p = num_frames - 151\n","\n","        #25\n","        q = random.randint(0,num_frames//15)\n","        sample_start_point = q\n","        if cal_fps > 30:\n","          sampling_rate = 20\n","        #sample(range(num_frames - (num_frames_to_extract * sampling_rate)), 1)[0]\n","    return sample_start_point, sampling_rate, num_frames\n","\n","########\n","def capture_and_sample_video(row_data_dir, video_name, num_frames_to_extract, sampling_rate, fps, save_path,\n","                             ucf101_fps, processing_mode):\n","    video = cv2.VideoCapture(os.path.join(row_data_dir, video_name))\n","    # if fps == 'Not known':\n","\n","    fps = video.get(cv2.CAP_PROP_FPS)\n","    ucf101_fps = video.get(cv2.CAP_PROP_FPS)\n","\n","    video_width = video.get(cv2.CAP_PROP_FRAME_WIDTH)\n","    video_height = video.get(cv2.CAP_PROP_FRAME_HEIGHT)\n","    \n","    print(\"video \", video_name)\n","\n","    sample_start_point, sampling_rate, num_frames = setting_sample_rate(num_frames_to_extract, sampling_rate, video,\n","                                                                        fps, ucf101_fps)\n","    # ====== setting the video to start reading from the frame we want ======\n","    \n","    print(\"details: \",sample_start_point, sampling_rate, num_frames )\n","   \n","\n","    image_array = []\n","    if num_frames_to_extract == 'all':\n","        num_frames_to_extract = int(num_frames / sampling_rate) if sampling_rate != 0 else num_frames\n","    # print(\"Nmber of frames to extract \", num_frames_to_extract)\n","    \n","    if processing_mode == 'live':\n","        transform = set_transforms(mode='test')\n","    for frame in range(num_frames_to_extract):\n","        video.set(1, sample_start_point)\n","        success, image = video.read()\n","        if not success:\n","            print('')#Error in reading frames from row video')\n","        else:\n","            # image = image[l:r, t:b] #top-botton, left-right\n","            RGB_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) if processing_mode == 'live' else image\n","            image = Image.fromarray(RGB_img.astype('uint8'), 'RGB')\n","            if processing_mode == 'live':\n","                image_array += [transform(image)]\n","            else:\n","                image_array += [np.uint8(image)]\n","        sample_start_point = sample_start_point + sampling_rate\n","    video.release()\n","    if processing_mode == 'main' and success:\n","        create_new_video(save_path, video_name, image_array)\n","\n","    else:\n","         print('Error in reading frames from row video')\n","    return image_array, [video_width, video_height]\n","\n","\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mln_qt6cs7T0"},"source":["You can change the preprocesing according to your requirements.\r\n","sampling rate and number of frames to exract are the important arguments which will change the model behaviour. Rename the output folder for every class.\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"a5Ga5u36bvhV","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1610564324013,"user_tz":360,"elapsed":11197,"user":{"displayName":"Roline Saldanha","photoUrl":"","userId":"08007127351234095824"}},"outputId":"7905178a-f26e-4956-ea43-110941af47f7"},"source":["import glob\n","import argparse\n","from tqdm import tqdm\n","from tqdm import tnrange, tqdm_notebook #used when I run in colab/GCloud\n","import os\n","\n","\n","parser = argparse.ArgumentParser(description='UCF101 Action Recognition preprocessing data, LRCN architecture')\n","\n","#path of the dataset\n","parser.add_argument('--row_data_dir', default=r'/content/drive/My Drive/directed studies/talk', type=str,\n","                    help='path to find the UCF101 row data')\n","                     type=str, help='path to find the UCF101 list splitting the data to train and test')\n","\n","parser.add_argument('--sampling_rate', default=5, type=int, help='how to sample the data')\n","parser.add_argument('--ucf101_fps', default=60, type=int, help='FPS of the UCF101 dataset')\n","parser.add_argument('--num_frames_to_extract', default=15, type=int, help='The number of frames what would be extracted from each video')\n","parser.add_argument('--video_file_name', default='y2mate.com - cute_happy_baby_crawling_BkJ6FJ2jJEQ_360p.mp4', type=str,\n","                    help='the video file name we would process, if none the script would run on all of the video files in the folder')\n","parser.add_argument('--dataset', default='UCF101', type=str,\n","                    help='the dataset name. options = youtube, UCF101')\n","\n","\n","def main_procesing_data(args, folder_dir, sampled_video_file=None, processing_mode='main'):\n","    \"\"\"\"\n","       Create the sampled data video,\n","       input - video, full length.\n","       function - 1. Read the video using CV2\n","                  2. from each video X (args.sampling_rate) frames are sampled reducing the FPS by args.sampling_rate (for example from 25 to 2.5 FPS)\n","                  3. The function randomly set the start point where the new sampled video would be read from, and Y(args.num_frames_to_extract) continues frames are extracted.\n","                  4. if processing_mode == 'main' The Y continues frames are extracted and save to a new video if not the data in tensor tyoe mode is passed to the next function\n","       Output: videos in length of X frames\n","       \"\"\"\n","    if args.dataset == 'UCF101':\n","        videopath = glob.glob(args.row_data_dir+\"/*\")\n","\n","        video_list = []\n","        for path in videopath:\n","          video_list.append(path)\n","        \n","        with tqdm(total=len(video_list)) as pbar:\n","        \n","            for video_name in video_list:\n","                print(video_name)\n","                video_name = video_name.split('/')[-1].rstrip('\\n')\n","                capture_and_sample_video(args.row_data_dir, video_name, args.num_frames_to_extract,\n","                                          args.sampling_rate, args.ucf101_fps, folder_dir,\n","                                          args.ucf101_fps, processing_mode)\n","                pbar.update(1)\n","           \n","\n","\n","args = parser.parse_args(\"\")\n","global_dir = \"/content/drive/My Drive/directed studies/\"\n","\n","#The folder name where the output will be stored\n","folder_name = 'testing{}_sampled_data_video_sampling_rate_{}_num frames extracted_{}'.format(args.dataset, args.sampling_rate, args.num_frames_to_extract)\n","\n","folder_dir = os.path.join(global_dir, folder_name)\n","create_folder_dir_if_needed(folder_dir)\n","save_setting_info(args, \"cpu\", folder_dir)\n","main_procesing_data(args, folder_dir)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/120 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["/content/drive/My Drive/directed studies/talk/American_History_X_talk_h_nm_np1_fr_goo_20.avi\n","video  American_History_X_talk_h_nm_np1_fr_goo_20.avi\n","FPS : 30.0\n","details:  8 5 231\n"],"name":"stdout"},{"output_type":"stream","text":["\r  1%|          | 1/120 [00:01<02:33,  1.29s/it]"],"name":"stderr"},{"output_type":"stream","text":["/content/drive/My Drive/directed studies/talk/American_History_X_talk_u_nm_np1_fr_goo_21.avi\n","video  American_History_X_talk_u_nm_np1_fr_goo_21.avi\n","FPS : 30.0\n","details:  2 5 117\n"],"name":"stdout"},{"output_type":"stream","text":["\r  2%|▏         | 2/120 [00:02<02:14,  1.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["/content/drive/My Drive/directed studies/talk/Faith_Rewarded_talk_h_nm_np1_fr_goo_46.avi\n","video  Faith_Rewarded_talk_h_nm_np1_fr_goo_46.avi\n","FPS : 30.0\n","details:  2 5 120\n"],"name":"stdout"},{"output_type":"stream","text":["\r  2%|▎         | 3/120 [00:02<02:05,  1.07s/it]"],"name":"stderr"},{"output_type":"stream","text":["/content/drive/My Drive/directed studies/talk/Faith_Rewarded_talk_h_nm_np1_fr_goo_65.avi\n","video  Faith_Rewarded_talk_h_nm_np1_fr_goo_65.avi\n","FPS : 30.0\n","details:  4 5 171\n"],"name":"stdout"},{"output_type":"stream","text":["\r  3%|▎         | 4/120 [00:03<01:56,  1.00s/it]"],"name":"stderr"},{"output_type":"stream","text":["/content/drive/My Drive/directed studies/talk/Faith_Rewarded_talk_h_nm_np1_fr_goo_90.avi\n","video  Faith_Rewarded_talk_h_nm_np1_fr_goo_90.avi\n","FPS : 30.0\n","details:  2 5 108\n"],"name":"stdout"},{"output_type":"stream","text":["\r  4%|▍         | 5/120 [00:04<01:50,  1.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["/content/drive/My Drive/directed studies/talk/Faith_Rewarded_talk_h_nm_np1_fr_goo_91.avi\n","video  Faith_Rewarded_talk_h_nm_np1_fr_goo_91.avi\n","FPS : 30.0\n","details:  3 5 100\n"],"name":"stdout"},{"output_type":"stream","text":["\r  5%|▌         | 6/120 [00:05<01:44,  1.09it/s]"],"name":"stderr"},{"output_type":"stream","text":["/content/drive/My Drive/directed studies/talk/Faith_Rewarded_talk_h_nm_np1_fr_goo_92.avi\n","video  Faith_Rewarded_talk_h_nm_np1_fr_goo_92.avi\n","FPS : 30.0\n","details:  5 5 164\n"],"name":"stdout"},{"output_type":"stream","text":["\r  6%|▌         | 7/120 [00:06<01:40,  1.12it/s]"],"name":"stderr"},{"output_type":"stream","text":["/content/drive/My Drive/directed studies/talk/Faith_Rewarded_talk_u_nm_np1_fr_goo_8.avi\n","video  Faith_Rewarded_talk_u_nm_np1_fr_goo_8.avi\n","FPS : 30.0\n","details:  5 5 105\n"],"name":"stdout"},{"output_type":"stream","text":["\r  7%|▋         | 8/120 [00:07<01:41,  1.11it/s]"],"name":"stderr"},{"output_type":"stream","text":["/content/drive/My Drive/directed studies/talk/Fellowship_2_talk_h_nm_np1_fr_bad_6.avi\n","video  Fellowship_2_talk_h_nm_np1_fr_bad_6.avi\n","FPS : 30.0\n","details:  6 5 168\n"],"name":"stdout"},{"output_type":"stream","text":["\r  8%|▊         | 9/120 [00:08<01:35,  1.16it/s]"],"name":"stderr"},{"output_type":"stream","text":["/content/drive/My Drive/directed studies/talk/Fellowship_3_talk_h_nm_np1_fr_goo_1.avi\n"],"name":"stdout"},{"output_type":"stream","text":["\r  8%|▊         | 9/120 [00:08<01:43,  1.08it/s]\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-ac72dad5c620>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mcreate_folder_dir_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0msave_setting_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mmain_procesing_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-4-ac72dad5c620>\u001b[0m in \u001b[0;36mmain_procesing_data\u001b[0;34m(args, folder_dir, sampled_video_file, processing_mode)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 capture_and_sample_video(args.row_data_dir, video_name, args.num_frames_to_extract,\n\u001b[1;32m     49\u001b[0m                                           \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampling_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mucf101_fps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                                           args.ucf101_fps, processing_mode)\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-f42c0488c1e1>\u001b[0m in \u001b[0;36mcapture_and_sample_video\u001b[0;34m(row_data_dir, video_name, num_frames_to_extract, sampling_rate, fps, save_path, ucf101_fps, processing_mode)\u001b[0m\n\u001b[1;32m    176\u001b[0m def capture_and_sample_video(row_data_dir, video_name, num_frames_to_extract, sampling_rate, fps, save_path,\n\u001b[1;32m    177\u001b[0m                              ucf101_fps, processing_mode):\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0;31m# if fps == 'Not known':\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"_pJavuzIPHWT"},"source":["After preprocessing, we get seperate folders. Copy all the smoking class videos into 1 folder and non smoking class video into another folder.\r\n","\r\n","Rename the videos using the code below"]},{"cell_type":"code","metadata":{"id":"obX30rERgETF","executionInfo":{"status":"ok","timestamp":1610566654221,"user_tz":360,"elapsed":351,"user":{"displayName":"Roline Saldanha","photoUrl":"","userId":"08007127351234095824"}}},"source":["#For smoking class\r\n","\r\n","import os \r\n","  \r\n","for count, filename in enumerate(os.listdir('/content/drive/My Drive/directed studies/Smoking_class_video_folder/')): \r\n","    if \".avi\" in filename:\r\n","      dst =\"1-Smoking\" + str(count) + \".avi\"\r\n","      src ='/content/drive/My Drive/directed studies/Smoking_class_video_folder/'+ filename \r\n","      dst ='/content/drive/My Drive/directed studies/Smoking_class_video_folder/'+ dst\r\n","        \r\n","      # rename() function will \r\n","      # rename all the files \r\n","      os.rename(src, dst) "],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"iHTcIctEQJp4"},"source":["#For Non-smoking class\r\n","\r\n","import os \r\n","  \r\n","for count, filename in enumerate(os.listdir('/content/drive/My Drive/directed studies/Non-Smoking_class_video_folder/')): \r\n","    if \".avi\" in filename:\r\n","      dst =\"1-Smoking\" + str(count) + \".avi\"\r\n","      src ='/content/drive/My Drive/directed studies/Non-Smoking_class_video_folder/'+ filename \r\n","      dst ='/content/drive/My Drive/directed studies/Non-Smoking_class_video_folder/'+ dst\r\n","        \r\n","      # rename() function will \r\n","      # rename all the files \r\n","      os.rename(src, dst) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IDkyB7R8XlIp"},"source":["Finally, Combine the videos of both classes to create the final dataset folder"]},{"cell_type":"code","metadata":{"id":"oLQZEKHlQodV"},"source":[""],"execution_count":null,"outputs":[]}]}